{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as activation_functions\n",
    "from torch.utils.data import default_collate\n",
    "import torch.nn as neural_network\n",
    "import torch.optim as optimization\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vit_b_32, ViT_B_32_Weights\n",
    "from torch import nn, optim\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from torch.nn import CrossEntropyLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !TODO: Random split 80/20 then 80>90/10\n",
    "# ! Metric: Accuracy, Precision, Recall, F1, AUC:\n",
    "# ! Accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
    "# ! Precision: TP / (TP + FP)\n",
    "# Macro average for equal class importance\n",
    "# ! AUC: discriminate between classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from torchmetrics) (24.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from torchmetrics) (2.2.2)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.2.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.10.0)\n",
      "Requirement already satisfied: filelock in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from torch>=1.10.0->torchmetrics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
      "Collecting fsspec (from torch>=1.10.0->torchmetrics)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Installing collected packages: lightning-utilities, fsspec, torchmetrics\n",
      "Successfully installed fsspec-2024.3.1 lightning-utilities-0.11.2 torchmetrics-1.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import torch.nn.functional as activation_functions\n",
    "# from torch.utils.data import default_collate\n",
    "# import torch.nn as neural_network\n",
    "# import torch.optim as optimization\n",
    "# import torch\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.models import vit_b_32, ViT_B_32_Weights\n",
    "# from torch import nn, optim\n",
    "# from PIL import Image, ImageDraw\n",
    "# import numpy as np\n",
    "# import argparse\n",
    "# import os\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# # Function for training the model\n",
    "# import torch.utils.data as data\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.utils.data as data\n",
    "\n",
    "# # ------------------------- STEP 1: Set up environment\n",
    "# # Set random seed for reproducibility\n",
    "# np.random.seed(32)\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# # Augmentation class\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class MixUp:\n",
    "#     \"\"\"\n",
    "#     MixUp Data Augmentation Class\n",
    "\n",
    "#     MixUp performs data augmentation by creating convex combinations of pairs of images and their labels,\n",
    "#     improving model generalization by encouraging linear behavior in-between training examples.\n",
    "\n",
    "#     Attributes:\n",
    "#     - mix_sampling_method (int): Determines the method for sampling the MixUp parameter λ.\n",
    "#         - 1: Sample λ from a Beta distribution with parameters (alpha, alpha).\n",
    "#         - 2: Sample λ uniformly from the range specified in 'uniform_range'.\n",
    "#     - alpha (float): The alpha parameter for the Beta distribution, relevant when mix_sampling_method is 1.\n",
    "#     - uniform_range (tuple of float): The range from which λ is uniformly sampled, relevant when mix_sampling_method is 2.\n",
    "#     - num_classes (int): The number of classes in the dataset, used for one-hot encoding the labels.\n",
    "\n",
    "#     Methods:\n",
    "#     - __call__(images, labels): Applies MixUp augmentation to a batch of images and labels.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self, mix_sampling_method=1, alpha=0.2, uniform_range=(0.0, 1.0), num_classes=10\n",
    "#     ):\n",
    "#         self.mix_sampling_method = mix_sampling_method\n",
    "#         self.alpha = alpha\n",
    "#         self.uniform_range = uniform_range\n",
    "#         self.num_classes = num_classes\n",
    "#         # Ensure reproducibility\n",
    "#         np.random.seed(42)  #! A ENLEVER\n",
    "#         torch.manual_seed(42)\n",
    "\n",
    "#     def __call__(self, images, labels):\n",
    "#         \"\"\"\n",
    "#         Apply MixUp augmentation to a batch of images and labels.\n",
    "\n",
    "#         Parameters:\n",
    "#         - images (Tensor): A batch of images.\n",
    "#         - labels (Tensor): Corresponding labels for the batch of images.\n",
    "\n",
    "#         Returns:\n",
    "#         - mixed_images (Tensor): Augmented images after applying MixUp.\n",
    "#         - mixed_labels (Tensor): Augmented labels after applying MixUp.\n",
    "#         \"\"\"\n",
    "#         batch_size = images.size(0)\n",
    "#         # Generate MixUp lambda parameter based on the specified sampling method\n",
    "#         if self.mix_sampling_method == 1:\n",
    "#             lam = np.random.beta(self.alpha, self.alpha, size=batch_size)\n",
    "#         else:  # uniform sampling\n",
    "#             lam = np.random.uniform(\n",
    "#                 self.uniform_range[0], self.uniform_range[1], size=batch_size\n",
    "#             )\n",
    "\n",
    "#         lam = torch.from_numpy(lam).float().to(images.device)\n",
    "#         lam = lam.view(batch_size, 1, 1, 1)\n",
    "#         index = torch.randperm(batch_size).to(images.device)\n",
    "\n",
    "#         mixed_images = lam * images + (1 - lam) * images[index, :]\n",
    "#         mixed_labels = self._mix_labels(labels, index, lam[:, 0, 0, 0])\n",
    "\n",
    "#         return mixed_images, mixed_labels\n",
    "\n",
    "#     def _mix_labels(self, labels, index, lam):\n",
    "#         \"\"\"\n",
    "#         Mix labels using the same lambda parameter used for mixing images.\n",
    "\n",
    "#         Parameters:\n",
    "#         - labels (Tensor): A batch of labels.\n",
    "#         - index (Tensor): A tensor of shuffled indices.\n",
    "#         - lam (Tensor): The lambda parameter used for mixing.\n",
    "\n",
    "#         Returns:\n",
    "#         - mixed_labels (Tensor): A tensor of mixed labels.\n",
    "#         \"\"\"\n",
    "#         one_hot_labels = F.one_hot(labels, num_classes=self.num_classes).float()\n",
    "#         return (\n",
    "#             lam.view(-1, 1) * one_hot_labels\n",
    "#             + (1 - lam.view(-1, 1)) * one_hot_labels[index]\n",
    "#         )\n",
    "\n",
    "\n",
    "# # Function for saving example images\n",
    "# def save_sample_images(\n",
    "#     image_transform,\n",
    "#     dataset_loader,\n",
    "#     category_names,\n",
    "#     save_directory,\n",
    "#     prediction_model=None,\n",
    "#     sample_count=16,\n",
    "# ):\n",
    "#     # ------------------------- STEP 2: Prepare data for visualization\n",
    "#     data_iterator = iter(dataset_loader)\n",
    "#     x_batch, y_batch = next(data_iterator)\n",
    "#     while len(x_batch) < sample_count:\n",
    "#         extra_x, extra_y = next(data_iterator)\n",
    "#         x_batch = torch.cat((x_batch, extra_x), dim=0)\n",
    "#         y_batch = torch.cat((y_batch, extra_y), dim=0)\n",
    "\n",
    "#     # --------------------------------------- Substep 2.1: Perform prediction if a model is provided\n",
    "#     if prediction_model is not None:\n",
    "#         model_outputs = prediction_model(x_batch)\n",
    "#         _, predictions = torch.max(model_outputs, 1)\n",
    "#         true_labels = [category_names[label] for label in y_batch]\n",
    "#         predicted_labels = [category_names[label] for label in predictions]\n",
    "\n",
    "#     mean_values = torch.tensor(image_transform.mean).view(3, 1, 1)\n",
    "#     std_values = torch.tensor(image_transform.std).view(3, 1, 1)\n",
    "\n",
    "#     image_width = x_batch[0].shape[1]\n",
    "#     image_height = x_batch[0].shape[2]\n",
    "#     grid_columns = 6\n",
    "#     grid_rows = sample_count // grid_columns + (1 if sample_count % grid_columns else 0)\n",
    "#     image_grid = Image.new(\n",
    "#         \"RGB\", (image_width * grid_columns, image_height * grid_rows), color=\"white\"\n",
    "#     )\n",
    "\n",
    "#     # ------------------------- STEP 3: Visualize and save images\n",
    "#     for i in range(sample_count):\n",
    "#         image_data = (x_batch[i] * std_values + mean_values).numpy()\n",
    "#         image_data = np.transpose(image_data, (1, 2, 0))\n",
    "#         img = Image.fromarray((image_data * 255).astype(np.uint8))\n",
    "\n",
    "#         # --------------------------------------- Substep 3.1: Annotate images with labels or predictions\n",
    "#         if prediction_model is None:\n",
    "#             label_list = [\n",
    "#                 category_names[idx] + \":\" + str(round(y_batch[i][idx].item(), 2))\n",
    "#                 for idx in y_batch[i].nonzero()\n",
    "#             ]\n",
    "#             img_title = f\"{', '.join(label_list)}\"\n",
    "#         else:\n",
    "#             img_title = f\"GT: {true_labels[i]}\\nPred: {predicted_labels[i]}\"\n",
    "#         draw = ImageDraw.Draw(img)\n",
    "#         draw.text((10, 10), img_title, fill=\"white\")\n",
    "\n",
    "#         # --------------------------------------- Substep 3.2: Compile images into a grid\n",
    "#         row_num, col_num = divmod(i, grid_columns)\n",
    "#         image_grid.paste(img, (col_num * image_width, row_num * image_height))\n",
    "\n",
    "#     # --------------------------------------- Substep 3.3: Save the grid image\n",
    "#     if not os.getcwd().split(os.sep)[-1].startswith(\"task\"):\n",
    "#         save_directory = \"task2/\" + save_directory\n",
    "#     image_grid.save(save_directory)\n",
    "#     print(f\"Image saved at {save_directory}\")\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.utils.data as data\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.utils.data as data\n",
    "# import torchmetrics\n",
    "\n",
    "\n",
    "# # Function for splitting the data into train, validation, and test sets\n",
    "# def split_data(data, labels, dev_ratio=0.8, val_ratio=0.1):\n",
    "\n",
    "#     num_samples = len(data)\n",
    "#     num_dev = int(num_samples * dev_ratio)\n",
    "#     num_val = int(num_dev * val_ratio)\n",
    "\n",
    "#     indices = torch.randperm(num_samples).tolist()\n",
    "#     dev_indices = indices[:num_dev]\n",
    "#     test_indices = indices[num_dev:]\n",
    "\n",
    "#     dev_data = data[dev_indices]\n",
    "#     dev_labels = labels[dev_indices]\n",
    "#     test_data = data[test_indices]\n",
    "#     test_labels = labels[test_indices]\n",
    "\n",
    "#     train_indices = dev_indices[:-num_val]\n",
    "#     val_indices = dev_indices[-num_val:]\n",
    "\n",
    "#     train_data = dev_data[:-num_val]\n",
    "#     train_labels = dev_labels[:-num_val]\n",
    "#     val_data = dev_data[-num_val:]\n",
    "#     val_labels = dev_labels[-num_val:]\n",
    "\n",
    "#     return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "# def perform_training(\n",
    "#     data,\n",
    "#     labels,\n",
    "#     vision_model,\n",
    "#     model_optimizer,\n",
    "#     loss_function,\n",
    "#     device,\n",
    "#     num_classes,\n",
    "#     num_epochs=20,\n",
    "#     sampling_method=1,\n",
    "# ):\n",
    "#     train_data, train_labels, val_data, val_labels, test_data, test_labels = split_data(\n",
    "#         data, labels\n",
    "#     )\n",
    "\n",
    "#     # train_dataset = data.TensorDataset(train_data, train_labels)\n",
    "#     train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "#     # train_loader = data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         train_dataset, batch_size=32, shuffle=True\n",
    "#     )\n",
    "#     val_dataset = torch.utils.data.TensorDataset(val_data, val_labels)\n",
    "#     # val_dataset = data.TensorDataset(val_data, val_labels)\n",
    "#     # val_loader = data.DataLoader(val_dataset, batch_size=32)\n",
    "#     val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "#     auroc = torchmetrics.AUROC(\n",
    "#         task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "#     )\n",
    "#     accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "#     vision_model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0.0\n",
    "#         for batch_index, (x_input, y_label) in enumerate(train_loader, 0):\n",
    "#             x_input, y_label = x_input.to(device), y_label.to(device)\n",
    "\n",
    "#             # MixUp augmentation\n",
    "#             mixup = MixUp(sampling_method)\n",
    "#             mixed_x, mixed_y = mixup(x_input, y_label)\n",
    "\n",
    "#             model_optimizer.zero_grad()\n",
    "\n",
    "#             model_output = vision_model(mixed_x)\n",
    "#             batch_loss = loss_function(model_output, mixed_y)\n",
    "#             batch_loss.backward()\n",
    "#             model_optimizer.step()\n",
    "\n",
    "#             total_loss += batch_loss.item()\n",
    "#             # Print after each batch (optional)\n",
    "#             print(\n",
    "#                 f\"Epoch: {epoch + 1}, Batch: {batch_index + 1}, Batch Loss: {batch_loss.item():.3f}\"\n",
    "#             )\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch: {epoch + 1}, Training Loss: {total_loss / len(train_loader):.3f}\"\n",
    "#         )\n",
    "\n",
    "#         vision_model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for x_val, y_val in val_loader:\n",
    "#                 x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "#                 val_output = vision_model(x_val)\n",
    "#                 auroc.update(val_output, y_val)\n",
    "#                 accuracy.update(val_output.argmax(dim=1), y_val)\n",
    "\n",
    "#         val_auc = auroc.compute()\n",
    "#         val_accuracy = accuracy.compute()\n",
    "#         print(\n",
    "#             f\"Validation AUC-ROC: {val_auc:.3f}, Validation Accuracy: {val_accuracy:.3f}\"\n",
    "#         )\n",
    "\n",
    "#         auroc.reset()\n",
    "#         accuracy.reset()\n",
    "\n",
    "#         vision_model.train()\n",
    "\n",
    "#     torch.save(vision_model.state_dict(), f\"model_{sampling_method}.pth\")\n",
    "#     print(\"Saved trained model\")\n",
    "\n",
    "\n",
    "# def test_model(test_loader, vision_model, device):\n",
    "#     vision_model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = vision_model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Accuracy of the model on the test images: {accuracy}%\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Set up device\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     import torchvision.transforms as transforms\n",
    "\n",
    "#     # Update the transform to include resizing\n",
    "#     transform = transforms.Compose(\n",
    "#         [\n",
    "#             transforms.Resize((224, 224)),  # Resize the images to 224x224\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(\n",
    "#                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "#             ),  # from https://pytorch.org/vision/stable/models.html\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     trainset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "#     trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "#     testset = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "#     testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "#     # # Data loading and transformation\n",
    "#     # trainset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "#     # trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "#     # testset = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "#     # testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "#     criterion = CrossEntropyLoss()\n",
    "\n",
    "#     num_epochs = 20\n",
    "\n",
    "#     # data = torch.from_numpy(trainloader.dataset.data).float()  # Convert data to tensor\n",
    "#     # labels = torch.tensor(trainloader.dataset.targets)  # Convert labels to a tensor\n",
    "\n",
    "#     # Extract data and labels outside the sampling loop\n",
    "#     data = torch.from_numpy(trainloader.dataset.data).float()  # Convert data to tensor\n",
    "#     d###########ata = transform(data)  # Apply the transform to resize the images #! Crame l'ordi\n",
    "#     labels = torch.tensor(trainloader.dataset.targets)  # Convert labels to a tensor\n",
    "\n",
    "#     for sampling_method in [1, 2]:\n",
    "#         # MixUp initialization\n",
    "#         mixup = MixUp(mix_sampling_method=sampling_method)\n",
    "\n",
    "#         # Model setup\n",
    "#         model = vit_b_32(weights=ViT_B_32_Weights.DEFAULT)\n",
    "#         model.heads = nn.Sequential(\n",
    "#             nn.Linear(model.heads[0].in_features, 10)\n",
    "#         )  # Adjusting for CIFAR-10\n",
    "#         model.to(device)\n",
    "\n",
    "#         # Loss and optimizer\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#         # Training\n",
    "#         perform_training(\n",
    "#             data,\n",
    "#             labels,\n",
    "#             model,\n",
    "#             optimizer,\n",
    "#             criterion,\n",
    "#             device,\n",
    "#             10,  # num_classes argument\n",
    "#             num_epochs,\n",
    "#             sampling_method,\n",
    "#         )\n",
    "\n",
    "#         # Testing\n",
    "#         test_model(testloader, model, device)\n",
    "#         save_sample_images(\n",
    "#             transform,\n",
    "#             testloader,\n",
    "#             category_names=[\n",
    "#                 \"plane\",\n",
    "#                 \"car\",\n",
    "#                 \"bird\",\n",
    "#                 \"cat\",\n",
    "#                 \"deer\",\n",
    "#                 \"dog\",\n",
    "#                 \"frog\",\n",
    "#                 \"horse\",\n",
    "#                 \"ship\",\n",
    "#                 \"truck\",\n",
    "#             ],\n",
    "#             save_directory=f\"result_{sampling_method}.png\",\n",
    "#             prediction_model=model,\n",
    "#             sample_count=36,\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch: 1, Batch: 1, Batch Loss: 2.380\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 361\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m    357\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    358\u001b[0m     [dataset\u001b[38;5;241m.\u001b[39mtargets[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_set\u001b[38;5;241m.\u001b[39mindices]\n\u001b[1;32m    359\u001b[0m )  \u001b[38;5;66;03m# Define labels for the training set\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m \u001b[43mperform_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# num_classes argument\u001b[39;49;00m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# Testing\u001b[39;00m\n\u001b[1;32m    374\u001b[0m test_model(test_loader, model, device)\n",
      "Cell \u001b[0;32mIn[8], line 249\u001b[0m, in \u001b[0;36mperform_training\u001b[0;34m(train_loader, val_loader, vision_model, model_optimizer, loss_function, device, num_classes, num_epochs, sampling_method)\u001b[0m\n\u001b[1;32m    247\u001b[0m model_output \u001b[38;5;241m=\u001b[39m vision_model(mixed_x)\n\u001b[1;32m    248\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m loss_function(model_output, mixed_y)\n\u001b[0;32m--> 249\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m model_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    252\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import torch.nn.functional as activation_functions\n",
    "# from torch.utils.data import default_collate\n",
    "# import torch.nn as neural_network\n",
    "# import torch.optim as optimization\n",
    "# import torch\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.models import vit_b_32, ViT_B_32_Weights\n",
    "# from torch import nn, optim\n",
    "# from PIL import Image, ImageDraw\n",
    "# import numpy as np\n",
    "# import argparse\n",
    "# import os\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# # Function for training the model\n",
    "# import torch.utils.data as data\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.utils.data as data\n",
    "\n",
    "# # ------------------------- STEP 1: Set up environment\n",
    "# # Set random seed for reproducibility\n",
    "# np.random.seed(32)\n",
    "# torch.manual_seed(42)\n",
    "# GEN_SEED = torch.Generator().manual_seed(42)\n",
    "\n",
    "# # Augmentation class\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class MixUp:\n",
    "#     \"\"\"\n",
    "#     MixUp Data Augmentation Class\n",
    "\n",
    "#     MixUp performs data augmentation by creating convex combinations of pairs of images and their labels,\n",
    "#     improving model generalization by encouraging linear behavior in-between training examples.\n",
    "\n",
    "#     Attributes:\n",
    "#     - mix_sampling_method (int): Determines the method for sampling the MixUp parameter λ.\n",
    "#         - 1: Sample λ from a Beta distribution with parameters (alpha, alpha).\n",
    "#         - 2: Sample λ uniformly from the range specified in 'uniform_range'.\n",
    "#     - alpha (float): The alpha parameter for the Beta distribution, relevant when mix_sampling_method is 1.\n",
    "#     - uniform_range (tuple of float): The range from which λ is uniformly sampled, relevant when mix_sampling_method is 2.\n",
    "#     - num_classes (int): The number of classes in the dataset, used for one-hot encoding the labels.\n",
    "\n",
    "#     Methods:\n",
    "#     - __call__(images, labels): Applies MixUp augmentation to a batch of images and labels.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self, mix_sampling_method=1, alpha=0.2, uniform_range=(0.0, 1.0), num_classes=10\n",
    "#     ):\n",
    "#         self.mix_sampling_method = mix_sampling_method\n",
    "#         self.alpha = alpha\n",
    "#         self.uniform_range = uniform_range\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#     def __call__(self, images, labels):\n",
    "#         \"\"\"\n",
    "#         Apply MixUp augmentation to a batch of images and labels.\n",
    "\n",
    "#         Parameters:\n",
    "#         - images (Tensor): A batch of images.\n",
    "#         - labels (Tensor): Corresponding labels for the batch of images.\n",
    "\n",
    "#         Returns:\n",
    "#         - mixed_images (Tensor): Augmented images after applying MixUp.\n",
    "#         - mixed_labels (Tensor): Augmented labels after applying MixUp.\n",
    "#         \"\"\"\n",
    "#         batch_size = images.size(0)\n",
    "#         # Generate MixUp lambda parameter based on the specified sampling method\n",
    "#         if self.mix_sampling_method == 1:\n",
    "#             lam = np.random.beta(self.alpha, self.alpha, size=batch_size)\n",
    "#         else:  # uniform sampling\n",
    "#             lam = np.random.uniform(\n",
    "#                 self.uniform_range[0], self.uniform_range[1], size=batch_size\n",
    "#             )\n",
    "\n",
    "#         lam = torch.from_numpy(lam).float().to(images.device)\n",
    "#         lam = lam.view(batch_size, 1, 1, 1)\n",
    "#         index = torch.randperm(batch_size).to(images.device)\n",
    "\n",
    "#         mixed_images = lam * images + (1 - lam) * images[index, :]\n",
    "#         mixed_labels = self._mix_labels(labels, index, lam[:, 0, 0, 0])\n",
    "\n",
    "#         return mixed_images, mixed_labels\n",
    "\n",
    "#     def _mix_labels(self, labels, index, lam):\n",
    "#         \"\"\"\n",
    "#         Mix labels using the same lambda parameter used for mixing images.\n",
    "\n",
    "#         Parameters:\n",
    "#         - labels (Tensor): A batch of labels.\n",
    "#         - index (Tensor): A tensor of shuffled indices.\n",
    "#         - lam (Tensor): The lambda parameter used for mixing.\n",
    "\n",
    "#         Returns:\n",
    "#         - mixed_labels (Tensor): A tensor of mixed labels.\n",
    "#         \"\"\"\n",
    "#         one_hot_labels = F.one_hot(labels, num_classes=self.num_classes).float()\n",
    "#         return (\n",
    "#             lam.view(-1, 1) * one_hot_labels\n",
    "#             + (1 - lam.view(-1, 1)) * one_hot_labels[index]\n",
    "#         )\n",
    "\n",
    "\n",
    "# # Function for saving example images\n",
    "# def save_sample_images(\n",
    "#     image_transform,\n",
    "#     dataset_loader,\n",
    "#     category_names,\n",
    "#     save_directory,\n",
    "#     prediction_model=None,\n",
    "#     sample_count=16,\n",
    "# ):\n",
    "#     # ------------------------- STEP 2: Prepare data for visualization\n",
    "#     data_iterator = iter(dataset_loader)\n",
    "#     x_batch, y_batch = next(data_iterator)\n",
    "#     while len(x_batch) < sample_count:\n",
    "#         extra_x, extra_y = next(data_iterator)\n",
    "#         x_batch = torch.cat((x_batch, extra_x), dim=0)\n",
    "#         y_batch = torch.cat((y_batch, extra_y), dim=0)\n",
    "\n",
    "#     # --------------------------------------- Substep 2.1: Perform prediction if a model is provided\n",
    "#     if prediction_model is not None:\n",
    "#         model_outputs = prediction_model(x_batch)\n",
    "#         _, predictions = torch.max(model_outputs, 1)\n",
    "#         true_labels = [category_names[label] for label in y_batch]\n",
    "#         predicted_labels = [category_names[label] for label in predictions]\n",
    "\n",
    "#     # mean_values = torch.tensor(image_transform.mean).view(3, 1, 1)\n",
    "#     # std_values = torch.tensor(image_transform.std).view(3, 1, 1)\n",
    "#     mean_values = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "#     std_values = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "#     image_width = x_batch[0].shape[1]\n",
    "#     image_height = x_batch[0].shape[2]\n",
    "#     grid_columns = 6\n",
    "#     grid_rows = sample_count // grid_columns + (1 if sample_count % grid_columns else 0)\n",
    "#     image_grid = Image.new(\n",
    "#         \"RGB\", (image_width * grid_columns, image_height * grid_rows), color=\"white\"\n",
    "#     )\n",
    "\n",
    "#     # ------------------------- STEP 3: Visualize and save images\n",
    "#     for i in range(sample_count):\n",
    "#         image_data = (x_batch[i] * std_values + mean_values).numpy()\n",
    "#         image_data = np.transpose(image_data, (1, 2, 0))\n",
    "#         img = Image.fromarray((image_data * 255).astype(np.uint8))\n",
    "\n",
    "#         # --------------------------------------- Substep 3.1: Annotate images with labels or predictions\n",
    "#         if prediction_model is None:\n",
    "#             label_list = [\n",
    "#                 category_names[idx] + \":\" + str(round(y_batch[i][idx].item(), 2))\n",
    "#                 for idx in y_batch[i].nonzero()\n",
    "#             ]\n",
    "#             img_title = f\"{', '.join(label_list)}\"\n",
    "#         else:\n",
    "#             img_title = f\"GT: {true_labels[i]}\\nPred: {predicted_labels[i]}\"\n",
    "#         draw = ImageDraw.Draw(img)\n",
    "#         draw.text((10, 10), img_title, fill=\"white\")\n",
    "\n",
    "#         # --------------------------------------- Substep 3.2: Compile images into a grid\n",
    "#         row_num, col_num = divmod(i, grid_columns)\n",
    "#         image_grid.paste(img, (col_num * image_width, row_num * image_height))\n",
    "\n",
    "#     # --------------------------------------- Substep 3.3: Save the grid image\n",
    "#     if not os.getcwd().split(os.sep)[-1].startswith(\"task\"):\n",
    "#         save_directory = \"task2/\" + save_directory\n",
    "#     image_grid.save(save_directory)\n",
    "#     print(f\"Image saved at {save_directory}\")\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.utils.data as data\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.utils.data as data\n",
    "# import torchmetrics\n",
    "\n",
    "\n",
    "# # Function for splitting the data into train, validation, and test sets\n",
    "# # def split_data(data, labels, dev_ratio=0.8, val_ratio=0.1):\n",
    "\n",
    "# #     num_samples = len(data)\n",
    "# #     num_dev = int(num_samples * dev_ratio)\n",
    "# #     num_val = int(num_dev * val_ratio)\n",
    "\n",
    "# #     indices = torch.randperm(num_samples).tolist()\n",
    "# #     dev_indices = indices[:num_dev]\n",
    "# #     test_indices = indices[num_dev:]\n",
    "\n",
    "# #     dev_data = data[dev_indices]\n",
    "# #     dev_labels = labels[dev_indices]\n",
    "# #     test_data = data[test_indices]\n",
    "# #     test_labels = labels[test_indices]\n",
    "\n",
    "# #     train_indices = dev_indices[:-num_val]\n",
    "# #     val_indices = dev_indices[-num_val:]\n",
    "\n",
    "# #     train_data = dev_data[:-num_val]\n",
    "# #     train_labels = dev_labels[:-num_val]\n",
    "# #     val_data = dev_data[-num_val:]\n",
    "# #     val_labels = dev_labels[-num_val:]\n",
    "\n",
    "# #     return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "# def perform_training(\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     vision_model,\n",
    "#     model_optimizer,\n",
    "#     loss_function,\n",
    "#     device,\n",
    "#     num_classes,\n",
    "#     num_epochs=20,\n",
    "#     sampling_method=1,\n",
    "# ):\n",
    "\n",
    "#     auroc = torchmetrics.AUROC(\n",
    "#         task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "#     )\n",
    "#     accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "#     vision_model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0.0\n",
    "#         for batch_index, (x_input, y_label) in enumerate(train_loader, 0):\n",
    "#             x_input, y_label = x_input.to(device), y_label.to(device)\n",
    "\n",
    "#             # MixUp augmentation\n",
    "#             mixup = MixUp(sampling_method)\n",
    "#             mixed_x, mixed_y = mixup(x_input, y_label)\n",
    "\n",
    "#             model_optimizer.zero_grad()\n",
    "\n",
    "#             model_output = vision_model(mixed_x)\n",
    "#             batch_loss = loss_function(model_output, mixed_y)\n",
    "#             batch_loss.backward()\n",
    "#             model_optimizer.step()\n",
    "\n",
    "#             total_loss += batch_loss.item()\n",
    "#             # Print after each batch (optional)\n",
    "#             print(\n",
    "#                 f\"Epoch: {epoch + 1}, Batch: {batch_index + 1}, Batch Loss: {batch_loss.item():.3f}\"\n",
    "#             )\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch: {epoch + 1}, Training Loss: {total_loss / len(train_loader):.3f}\"\n",
    "#         )\n",
    "\n",
    "#         vision_model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             for x_val, y_val in val_loader:\n",
    "#                 x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "#                 val_output = vision_model(x_val)\n",
    "#                 auroc.update(val_output, y_val)\n",
    "#                 accuracy.update(val_output.argmax(dim=1), y_val)\n",
    "\n",
    "#         val_auc = auroc.compute()\n",
    "#         val_accuracy = accuracy.compute()\n",
    "#         print(\n",
    "#             f\"Validation AUC-ROC: {val_auc:.3f}, Validation Accuracy: {val_accuracy:.3f}\"\n",
    "#         )\n",
    "\n",
    "#         auroc.reset()\n",
    "#         accuracy.reset()\n",
    "\n",
    "#         vision_model.train()\n",
    "\n",
    "#     torch.save(vision_model.state_dict(), f\"model_{sampling_method}.pth\")\n",
    "#     print(\"Saved trained model\")\n",
    "\n",
    "\n",
    "# def test_model(test_loader, vision_model, device):\n",
    "#     vision_model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = vision_model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Accuracy of the model on the test images: {accuracy}%\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Set up device\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     import torchvision.transforms as transforms\n",
    "\n",
    "#     # Update the transform to include resizing\n",
    "#     transform = transforms.Compose(\n",
    "#         [\n",
    "#             transforms.Resize((224, 224)),  # Resize the images to 224x224\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(\n",
    "#                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "#             ),  # from https://pytorch.org/vision/stable/models.html\n",
    "#         ]\n",
    "#     )\n",
    "#     # ...\n",
    "\n",
    "#     # Load CIFAR-10 train dataset and further split it into test and validation\n",
    "#     dataset = torchvision.datasets.CIFAR10(\n",
    "#         root=\"./data\", train=True, download=True, transform=transform\n",
    "#     )\n",
    "\n",
    "#     #  Split\n",
    "#     train_set, test_set = torch.utils.data.random_split(\n",
    "#         dataset, [0.8, 0.2], generator=GEN_SEED\n",
    "#     )\n",
    "#     train_set, val_set = torch.utils.data.random_split(\n",
    "#         train_set, [0.9, 0.1], generator=GEN_SEED\n",
    "#     )\n",
    "\n",
    "#     # DataLoaders\n",
    "#     train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "#     val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "#     test_loader = DataLoader(train_set, batch_size=32, shuffle=False)\n",
    "\n",
    "#     criterion = CrossEntropyLoss()\n",
    "\n",
    "#     num_epochs = 20\n",
    "\n",
    "#     for sampling_method in [1, 2]:\n",
    "#         # MixUp initialization\n",
    "#         mixup = MixUp(mix_sampling_method=sampling_method)\n",
    "\n",
    "#         # Model setup\n",
    "#         model = vit_b_32(weights=ViT_B_32_Weights.DEFAULT)\n",
    "#         model.heads = nn.Sequential(\n",
    "#             nn.Linear(model.heads[0].in_features, 10)\n",
    "#         )  # Adjusting for CIFAR-10\n",
    "#         model.to(device)\n",
    "\n",
    "#         # Loss and optimizer\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#         # Training\n",
    "#         labels = torch.tensor(\n",
    "#             [dataset.targets[i] for i in train_set.indices]\n",
    "#         )  # Define labels for the training set\n",
    "\n",
    "#         perform_training(\n",
    "#             train_loader,\n",
    "#             val_loader,\n",
    "#             model,\n",
    "#             optimizer,\n",
    "#             criterion,\n",
    "#             device,\n",
    "#             10,  # num_classes argument\n",
    "#             num_epochs,\n",
    "#             sampling_method,\n",
    "#         )\n",
    "\n",
    "#         # Testing\n",
    "#         test_model(test_loader, model, device)\n",
    "#         save_sample_images(\n",
    "#             transform,\n",
    "#             test_loader,\n",
    "#             category_names=[\n",
    "#                 \"plane\",\n",
    "#                 \"car\",\n",
    "#                 \"bird\",\n",
    "#                 \"cat\",\n",
    "#                 \"deer\",\n",
    "#                 \"dog\",\n",
    "#                 \"frog\",\n",
    "#                 \"horse\",\n",
    "#                 \"ship\",\n",
    "#                 \"truck\",\n",
    "#             ],\n",
    "#             save_directory=f\"result_{sampling_method}.png\",\n",
    "#             prediction_model=model,\n",
    "#             sample_count=36,\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Number of batches in train_loader: 1125\n",
      "Number of batches in val_loader: 125\n",
      "Number of batches in test_loader: 1125\n",
      "Epoch: 1, Batch: 1, Batch Loss: 2.380\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# import torch.nn.functional as activation_functions\n",
    "# from torch.utils.data import default_collate\n",
    "# import torch.nn as neural_network\n",
    "# import torch.optim as optimization\n",
    "# import torch\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.models import vit_b_32, ViT_B_32_Weights\n",
    "# from torch import nn, optim\n",
    "# from PIL import Image, ImageDraw\n",
    "# import numpy as np\n",
    "# import argparse\n",
    "# import os\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "# import torch\n",
    "# import torchvision\n",
    "# import torch.nn.functional as activation_functions\n",
    "# from torch.utils.data import default_collate\n",
    "# import torch.nn as neural_network\n",
    "# import torch.optim as optimization\n",
    "# import torch\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.models import vit_b_32, ViT_B_32_Weights\n",
    "# from torch import nn, optim\n",
    "# from PIL import Image, ImageDraw\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from PIL import Image, ImageDraw\n",
    "# import numpy as np\n",
    "# import argparse\n",
    "# import os\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "# # Function for training the model\n",
    "# import torch.utils.data as data\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.utils.data as data\n",
    "\n",
    "# # ------------------------- STEP 1: Set up environment\n",
    "# # Set random seed for reproducibility\n",
    "# np.random.seed(32)\n",
    "# torch.manual_seed(42)\n",
    "# GEN_SEED = torch.Generator().manual_seed(42)\n",
    "\n",
    "# # Augmentation class\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class MixUp:\n",
    "#     \"\"\"\n",
    "#     MixUp Data Augmentation Class\n",
    "\n",
    "#     MixUp performs data augmentation by creating convex combinations of pairs of images and their labels,\n",
    "#     improving model generalization by encouraging linear behavior in-between training examples.\n",
    "\n",
    "#     Attributes:\n",
    "#     - mix_sampling_method (int): Determines the method for sampling the MixUp parameter λ.\n",
    "#         - 1: Sample λ from a Beta distribution with parameters (alpha, alpha).\n",
    "#         - 2: Sample λ uniformly from the range specified in 'uniform_range'.\n",
    "#     - alpha (float): The alpha parameter for the Beta distribution, relevant when mix_sampling_method is 1.\n",
    "#     - uniform_range (tuple of float): The range from which λ is uniformly sampled, relevant when mix_sampling_method is 2.\n",
    "#     - num_classes (int): The number of classes in the dataset, used for one-hot encoding the labels.\n",
    "\n",
    "#     Methods:\n",
    "#     - __call__(images, labels): Applies MixUp augmentation to a batch of images and labels.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self, mix_sampling_method=1, alpha=0.2, uniform_range=(0.0, 1.0), num_classes=10\n",
    "#     ):\n",
    "#         self.mix_sampling_method = mix_sampling_method\n",
    "#         self.alpha = alpha\n",
    "#         self.uniform_range = uniform_range\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#     def __call__(self, images, labels):\n",
    "#         \"\"\"\n",
    "#         Apply MixUp augmentation to a batch of images and labels.\n",
    "\n",
    "#         Parameters:\n",
    "#         - images (Tensor): A batch of images.\n",
    "#         - labels (Tensor): Corresponding labels for the batch of images.\n",
    "\n",
    "#         Returns:\n",
    "#         - mixed_images (Tensor): Augmented images after applying MixUp.\n",
    "#         - mixed_labels (Tensor): Augmented labels after applying MixUp.\n",
    "#         \"\"\"\n",
    "#         batch_size = images.size(0)\n",
    "#         # Generate MixUp lambda parameter based on the specified sampling method\n",
    "#         if self.mix_sampling_method == 1:\n",
    "#             lam = np.random.beta(self.alpha, self.alpha, size=batch_size)\n",
    "#         else:  # uniform sampling\n",
    "#             lam = np.random.uniform(\n",
    "#                 self.uniform_range[0], self.uniform_range[1], size=batch_size\n",
    "#             )\n",
    "\n",
    "#         lam = torch.from_numpy(lam).float().to(images.device)\n",
    "#         lam = lam.view(batch_size, 1, 1, 1)\n",
    "#         index = torch.randperm(batch_size).to(images.device)\n",
    "\n",
    "#         mixed_images = lam * images + (1 - lam) * images[index, :]\n",
    "#         mixed_labels = self._mix_labels(labels, index, lam[:, 0, 0, 0])\n",
    "\n",
    "#         return mixed_images, mixed_labels\n",
    "\n",
    "#     def _mix_labels(self, labels, index, lam):\n",
    "#         \"\"\"\n",
    "#         Mix labels using the same lambda parameter used for mixing images.\n",
    "\n",
    "#         Parameters:\n",
    "#         - labels (Tensor): A batch of labels.\n",
    "#         - index (Tensor): A tensor of shuffled indices.\n",
    "#         - lam (Tensor): The lambda parameter used for mixing.\n",
    "\n",
    "#         Returns:\n",
    "#         - mixed_labels (Tensor): A tensor of mixed labels.\n",
    "#         \"\"\"\n",
    "#         one_hot_labels = F.one_hot(labels, num_classes=self.num_classes).float()\n",
    "#         return (\n",
    "#             lam.view(-1, 1) * one_hot_labels\n",
    "#             + (1 - lam.view(-1, 1)) * one_hot_labels[index]\n",
    "#         )\n",
    "\n",
    "\n",
    "# # Function for saving example images\n",
    "# def save_sample_images(\n",
    "#     image_transform,\n",
    "#     dataset_loader,\n",
    "#     category_names,\n",
    "#     save_directory,\n",
    "#     prediction_model=None,\n",
    "#     sample_count=16,\n",
    "# ):\n",
    "#     # ------------------------- STEP 2: Prepare data for visualization\n",
    "#     data_iterator = iter(dataset_loader)\n",
    "#     x_batch, y_batch = next(data_iterator)\n",
    "#     while len(x_batch) < sample_count:\n",
    "#         extra_x, extra_y = next(data_iterator)\n",
    "#         x_batch = torch.cat((x_batch, extra_x), dim=0)\n",
    "#         y_batch = torch.cat((y_batch, extra_y), dim=0)\n",
    "\n",
    "#     # --------------------------------------- Substep 2.1: Perform prediction if a model is provided\n",
    "#     if prediction_model is not None:\n",
    "#         model_outputs = prediction_model(x_batch)\n",
    "#         _, predictions = torch.max(model_outputs, 1)\n",
    "#         true_labels = [category_names[label] for label in y_batch]\n",
    "#         predicted_labels = [category_names[label] for label in predictions]\n",
    "\n",
    "#     # mean_values = torch.tensor(image_transform.mean).view(3, 1, 1)\n",
    "#     # std_values = torch.tensor(image_transform.std).view(3, 1, 1)\n",
    "#     mean_values = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "#     std_values = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "#     image_width = x_batch[0].shape[1]\n",
    "#     image_height = x_batch[0].shape[2]\n",
    "#     grid_columns = 6\n",
    "#     grid_rows = sample_count // grid_columns + (1 if sample_count % grid_columns else 0)\n",
    "#     image_grid = Image.new(\n",
    "#         \"RGB\", (image_width * grid_columns, image_height * grid_rows), color=\"white\"\n",
    "#     )\n",
    "\n",
    "#     # ------------------------- STEP 3: Visualize and save images\n",
    "#     for i in range(sample_count):\n",
    "#         image_data = (x_batch[i] * std_values + mean_values).numpy()\n",
    "#         image_data = np.transpose(image_data, (1, 2, 0))\n",
    "#         img = Image.fromarray((image_data * 255).astype(np.uint8))\n",
    "\n",
    "#         # --------------------------------------- Substep 3.1: Annotate images with labels or predictions\n",
    "#         if prediction_model is None:\n",
    "#             label_list = [\n",
    "#                 category_names[idx] + \":\" + str(round(y_batch[i][idx].item(), 2))\n",
    "#                 for idx in y_batch[i].nonzero()\n",
    "#             ]\n",
    "#             img_title = f\"{', '.join(label_list)}\"\n",
    "#         else:\n",
    "#             img_title = f\"GT: {true_labels[i]}\\nPred: {predicted_labels[i]}\"\n",
    "#         draw = ImageDraw.Draw(img)\n",
    "#         draw.text((10, 10), img_title, fill=\"white\")\n",
    "\n",
    "#         # --------------------------------------- Substep 3.2: Compile images into a grid\n",
    "#         row_num, col_num = divmod(i, grid_columns)\n",
    "#         image_grid.paste(img, (col_num * image_width, row_num * image_height))\n",
    "\n",
    "#     # --------------------------------------- Substep 3.3: Save the grid image\n",
    "#     if not os.getcwd().split(os.sep)[-1].startswith(\"task\"):\n",
    "#         save_directory = \"task2/\" + save_directory\n",
    "#     image_grid.save(save_directory)\n",
    "#     print(f\"Image saved at {save_directory}\")\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.utils.data as data\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.utils.data as data\n",
    "# import torchmetrics\n",
    "\n",
    "\n",
    "# # Function for splitting the data into train, validation, and test sets\n",
    "# # def split_data(data, labels, dev_ratio=0.8, val_ratio=0.1):\n",
    "\n",
    "# #     num_samples = len(data)\n",
    "# #     num_dev = int(num_samples * dev_ratio)\n",
    "# #     num_val = int(num_dev * val_ratio)\n",
    "\n",
    "# #     indices = torch.randperm(num_samples).tolist()\n",
    "# #     dev_indices = indices[:num_dev]\n",
    "# #     test_indices = indices[num_dev:]\n",
    "\n",
    "# #     dev_data = data[dev_indices]\n",
    "# #     dev_labels = labels[dev_indices]\n",
    "# #     test_data = data[test_indices]\n",
    "# #     test_labels = labels[test_indices]\n",
    "\n",
    "# #     train_indices = dev_indices[:-num_val]\n",
    "# #     val_indices = dev_indices[-num_val:]\n",
    "\n",
    "# #     train_data = dev_data[:-num_val]\n",
    "# #     train_labels = dev_labels[:-num_val]\n",
    "# #     val_data = dev_data[-num_val:]\n",
    "# #     val_labels = dev_labels[-num_val:]\n",
    "\n",
    "# #     return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "# def perform_training(\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     vision_model,\n",
    "#     model_optimizer,\n",
    "#     loss_function,\n",
    "#     device,\n",
    "#     num_classes,\n",
    "#     num_epochs=20,\n",
    "#     sampling_method=1,\n",
    "# ):\n",
    "\n",
    "#     auroc = torchmetrics.AUROC(\n",
    "#         task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "#     )\n",
    "#     accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "#     best_val_auc = 0.0\n",
    "#     best_val_accuracy = 0.0\n",
    "#     best_epoch = 0\n",
    "\n",
    "#     vision_model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0.0\n",
    "#         for batch_index, (x_input, y_label) in enumerate(train_loader, 0):\n",
    "#             x_input, y_label = x_input.to(device), y_label.to(device)\n",
    "\n",
    "#             # MixUp augmentation\n",
    "#             mixup = MixUp(sampling_method)\n",
    "#             mixed_x, mixed_y = mixup(x_input, y_label)\n",
    "\n",
    "#             model_optimizer.zero_grad()\n",
    "\n",
    "#             model_output = vision_model(mixed_x)\n",
    "#             batch_loss = loss_function(model_output, mixed_y)\n",
    "#             batch_loss.backward()\n",
    "#             model_optimizer.step()\n",
    "\n",
    "#             total_loss += batch_loss.item()\n",
    "#             # Print after each batch (optional)\n",
    "#             print(\n",
    "#                 f\"Epoch: {epoch + 1}, Batch: {batch_index + 1}, Batch Loss: {batch_loss.item():.3f}\"\n",
    "#             )\n",
    "\n",
    "#         print(\n",
    "#             f\"Epoch: {epoch + 1}, Training Loss: {total_loss / len(train_loader):.3f}\"\n",
    "#         )\n",
    "\n",
    "#         vision_model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             val_loss = 0.0\n",
    "#             for x_val, y_val in val_loader:\n",
    "#                 x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "#                 val_output = vision_model(x_val)\n",
    "#                 val_loss += loss_function(val_output, y_val).item()\n",
    "#                 auroc.update(val_output, y_val)\n",
    "#                 accuracy.update(val_output.argmax(dim=1), y_val)\n",
    "\n",
    "#         val_loss /= len(val_loader)\n",
    "#         val_auc = auroc.compute()\n",
    "#         val_accuracy = accuracy.compute()\n",
    "\n",
    "#         # Update the best metrics if the current epoch has better performance\n",
    "#         if val_auc > best_val_auc:\n",
    "#             best_val_auc = val_auc\n",
    "#             best_val_accuracy = val_accuracy\n",
    "#             best_epoch = epoch + 1\n",
    "\n",
    "#         # Print the validation metrics for the current epoch\n",
    "#         print(\n",
    "#             f\"Validation Loss: {val_loss:.3f}, \"\n",
    "#             f\"Validation AUC-ROC: {val_auc:.3f}, \"\n",
    "#             f\"Validation Accuracy: {val_accuracy:.3f}\"\n",
    "#         )\n",
    "\n",
    "#         auroc.reset()\n",
    "#         accuracy.reset()\n",
    "\n",
    "#         vision_model.train()\n",
    "\n",
    "#     # Print the summary of the best performance\n",
    "#     print(f\"\\nBest Performance (Epoch {best_epoch}):\")\n",
    "#     print(f\"Validation AUC-ROC: {best_val_auc:.3f}\")\n",
    "#     print(f\"Validation Accuracy: {best_val_accuracy:.3f}\")\n",
    "\n",
    "#     torch.save(vision_model.state_dict(), f\"model_{sampling_method}.pth\")\n",
    "#     print(\"Saved trained model\")\n",
    "\n",
    "\n",
    "# def test_model(test_loader, vision_model, loss_function, device, num_classes):\n",
    "#     vision_model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     test_auroc = torchmetrics.AUROC(task=\"multiclass\", num_classes=num_classes)\n",
    "#     test_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = vision_model(images)\n",
    "#             test_loss += loss_function(outputs, labels).item()\n",
    "#             test_auroc.update(outputs, labels)\n",
    "#             test_accuracy.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "#     test_loss /= len(test_loader)\n",
    "#     test_auroc_score = test_auroc.compute()\n",
    "#     test_accuracy_score = test_accuracy.compute()\n",
    "\n",
    "#     return test_loss, test_auroc_score, test_accuracy_score\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Set up device\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     import torchvision.transforms as transforms\n",
    "\n",
    "#     # Update the transform to include resizing\n",
    "#     transform = transforms.Compose(\n",
    "#         [\n",
    "#             transforms.Resize((224, 224)),  # Resize the images to 224x224\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(\n",
    "#                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "#             ),  # from https://pytorch.org/vision/stable/models.html\n",
    "#         ]\n",
    "#     )\n",
    "#     # ...\n",
    "\n",
    "#     # Load CIFAR-10 train dataset and further split it into test and validation\n",
    "#     dataset = torchvision.datasets.CIFAR10(\n",
    "#         root=\"./data\", train=True, download=True, transform=transform\n",
    "#     )\n",
    "\n",
    "#     #  Split\n",
    "#     train_set, test_set = torch.utils.data.random_split(\n",
    "#         dataset, [0.8, 0.2], generator=GEN_SEED\n",
    "#     )\n",
    "#     train_set, val_set = torch.utils.data.random_split(\n",
    "#         train_set, [0.9, 0.1], generator=GEN_SEED\n",
    "#     )\n",
    "\n",
    "#     # DataLoaders\n",
    "#     train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "#     val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "#     test_loader = DataLoader(train_set, batch_size=32, shuffle=False)\n",
    "\n",
    "#     # Print the number of batches in each data loader\n",
    "#     print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "#     print(f\"Number of batches in val_loader: {len(val_loader)}\")\n",
    "#     print(f\"Number of batches in test_loader: {len(test_loader)}\")\n",
    "\n",
    "#     criterion = CrossEntropyLoss()\n",
    "\n",
    "#     num_epochs = 20\n",
    "\n",
    "#     for sampling_method in [1, 2]:\n",
    "#         # MixUp initialization\n",
    "#         mixup = MixUp(mix_sampling_method=sampling_method)\n",
    "\n",
    "#         # Model setup\n",
    "#         model = vit_b_32(weights=ViT_B_32_Weights.DEFAULT)\n",
    "#         model.heads = nn.Sequential(\n",
    "#             nn.Linear(model.heads[0].in_features, 10)\n",
    "#         )  # Adjusting for CIFAR-10\n",
    "#         model.to(device)\n",
    "\n",
    "#         # Loss and optimizer\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#         best_val_loss = float(\"inf\")\n",
    "#         best_val_auc = 0.0\n",
    "#         best_val_accuracy = 0.0\n",
    "\n",
    "#         # Training\n",
    "#         labels = torch.tensor(\n",
    "#             [dataset.targets[i] for i in train_set.indices]\n",
    "#         )  # Define labels for the training set\n",
    "\n",
    "#         perform_training(\n",
    "#             train_loader,\n",
    "#             val_loader,\n",
    "#             model,\n",
    "#             optimizer,\n",
    "#             criterion,\n",
    "#             device,\n",
    "#             10,  # num_classes argument\n",
    "#             num_epochs,\n",
    "#             sampling_method,\n",
    "#         )\n",
    "\n",
    "#         # Testing\n",
    "#         test_loss, test_auc, test_accuracy = test_model(\n",
    "#             test_loader, model, criterion, device, 10\n",
    "#         )\n",
    "\n",
    "#         # Print the summary of loss values and metrics on the holdout test set\n",
    "#         print(f\"\\nSampling Method: {sampling_method}\")\n",
    "#         print(f\"Test Loss: {test_loss:.3f}\")\n",
    "#         print(f\"Test AUC-ROC: {test_auc:.3f}\")\n",
    "#         print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "#         # Compare the results with those obtained during development\n",
    "#         print(f\"\\nComparison with Development Results:\")\n",
    "#         print(f\"Best Validation Loss: {best_val_loss:.3f}\")\n",
    "#         print(f\"Best Validation AUC-ROC: {best_val_auc:.3f}\")\n",
    "#         print(f\"Best Validation Accuracy: {best_val_accuracy:.3f}\")\n",
    "\n",
    "#         save_sample_images(\n",
    "#             transform,\n",
    "#             test_loader,\n",
    "#             category_names=[\n",
    "#                 \"plane\",\n",
    "#                 \"car\",\n",
    "#                 \"bird\",\n",
    "#                 \"cat\",\n",
    "#                 \"deer\",\n",
    "#                 \"dog\",\n",
    "#                 \"frog\",\n",
    "#                 \"horse\",\n",
    "#                 \"ship\",\n",
    "#                 \"truck\",\n",
    "#             ],\n",
    "#             save_directory=f\"result_{sampling_method}.png\",\n",
    "#             prediction_model=model,\n",
    "#             sample_count=36,\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Number of batches in train_loader: 1125\n",
      "Number of batches in val_loader: 125\n",
      "Number of batches in test_loader: 1125\n",
      "Epoch: 1, Batch: 1, Batch Loss: 2.380\n",
      "Epoch: 1, Batch: 2, Batch Loss: 2.346\n",
      "Epoch: 1, Batch: 3, Batch Loss: 2.251\n",
      "Epoch: 1, Batch: 4, Batch Loss: 2.264\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 402\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m    398\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    399\u001b[0m         [dataset\u001b[38;5;241m.\u001b[39mtargets[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_set\u001b[38;5;241m.\u001b[39mindices]\n\u001b[1;32m    400\u001b[0m     )  \u001b[38;5;66;03m# Define labels for the training set\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     best_val_loss, best_val_auc, best_val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mperform_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# num_classes argument\u001b[39;49;00m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampling_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# Testing\u001b[39;00m\n\u001b[1;32m    415\u001b[0m test_loss, test_auc, test_accuracy \u001b[38;5;241m=\u001b[39m test_model(\n\u001b[1;32m    416\u001b[0m     test_loader, model, criterion, device, \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    417\u001b[0m )\n",
      "Cell \u001b[0;32mIn[1], line 252\u001b[0m, in \u001b[0;36mperform_training\u001b[0;34m(train_loader, val_loader, vision_model, model_optimizer, loss_function, device, num_classes, num_epochs, sampling_method)\u001b[0m\n\u001b[1;32m    250\u001b[0m model_output \u001b[38;5;241m=\u001b[39m vision_model(mixed_x)\n\u001b[1;32m    251\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m loss_function(model_output, mixed_y)\n\u001b[0;32m--> 252\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m model_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    255\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as activation_functions\n",
    "from torch.utils.data import default_collate\n",
    "import torch.nn as neural_network\n",
    "import torch.optim as optimization\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import vit_b_32, ViT_B_32_Weights\n",
    "from torch import nn, optim\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Function for training the model\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "# ------------------------- STEP 1: Set up environment\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(32)\n",
    "torch.manual_seed(42)\n",
    "GEN_SEED = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Augmentation class\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MixUp:\n",
    "    \"\"\"\n",
    "    MixUp Data Augmentation Class\n",
    "\n",
    "    MixUp performs data augmentation by creating convex combinations of pairs of images and their labels,\n",
    "    improving model generalization by encouraging linear behavior in-between training examples.\n",
    "\n",
    "    Attributes:\n",
    "    - mix_sampling_method (int): Determines the method for sampling the MixUp parameter λ.\n",
    "        - 1: Sample λ from a Beta distribution with parameters (alpha, alpha).\n",
    "        - 2: Sample λ uniformly from the range specified in 'uniform_range'.\n",
    "    - alpha (float): The alpha parameter for the Beta distribution, relevant when mix_sampling_method is 1.\n",
    "    - uniform_range (tuple of float): The range from which λ is uniformly sampled, relevant when mix_sampling_method is 2.\n",
    "    - num_classes (int): The number of classes in the dataset, used for one-hot encoding the labels.\n",
    "\n",
    "    Methods:\n",
    "    - __call__(images, labels): Applies MixUp augmentation to a batch of images and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, mix_sampling_method=1, alpha=0.2, uniform_range=(0.0, 1.0), num_classes=10\n",
    "    ):\n",
    "        self.mix_sampling_method = mix_sampling_method\n",
    "        self.alpha = alpha\n",
    "        self.uniform_range = uniform_range\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, images, labels):\n",
    "        \"\"\"\n",
    "        Apply MixUp augmentation to a batch of images and labels.\n",
    "\n",
    "        Parameters:\n",
    "        - images (Tensor): A batch of images.\n",
    "        - labels (Tensor): Corresponding labels for the batch of images.\n",
    "\n",
    "        Returns:\n",
    "        - mixed_images (Tensor): Augmented images after applying MixUp.\n",
    "        - mixed_labels (Tensor): Augmented labels after applying MixUp.\n",
    "        \"\"\"\n",
    "        batch_size = images.size(0)\n",
    "        # Generate MixUp lambda parameter based on the specified sampling method\n",
    "        if self.mix_sampling_method == 1:\n",
    "            lam = np.random.beta(self.alpha, self.alpha, size=batch_size)\n",
    "        else:  # uniform sampling\n",
    "            lam = np.random.uniform(\n",
    "                self.uniform_range[0], self.uniform_range[1], size=batch_size\n",
    "            )\n",
    "\n",
    "        lam = torch.from_numpy(lam).float().to(images.device)\n",
    "        lam = lam.view(batch_size, 1, 1, 1)\n",
    "        index = torch.randperm(batch_size).to(images.device)\n",
    "\n",
    "        mixed_images = lam * images + (1 - lam) * images[index, :]\n",
    "        mixed_labels = self._mix_labels(labels, index, lam[:, 0, 0, 0])\n",
    "\n",
    "        return mixed_images, mixed_labels\n",
    "\n",
    "    def _mix_labels(self, labels, index, lam):\n",
    "        \"\"\"\n",
    "        Mix labels using the same lambda parameter used for mixing images.\n",
    "\n",
    "        Parameters:\n",
    "        - labels (Tensor): A batch of labels.\n",
    "        - index (Tensor): A tensor of shuffled indices.\n",
    "        - lam (Tensor): The lambda parameter used for mixing.\n",
    "\n",
    "        Returns:\n",
    "        - mixed_labels (Tensor): A tensor of mixed labels.\n",
    "        \"\"\"\n",
    "        one_hot_labels = F.one_hot(labels, num_classes=self.num_classes).float()\n",
    "        return (\n",
    "            lam.view(-1, 1) * one_hot_labels\n",
    "            + (1 - lam.view(-1, 1)) * one_hot_labels[index]\n",
    "        )\n",
    "\n",
    "\n",
    "# Function for saving example images\n",
    "def save_sample_images(\n",
    "    image_transform,\n",
    "    dataset_loader,\n",
    "    category_names,\n",
    "    save_directory,\n",
    "    prediction_model=None,\n",
    "    sample_count=16,\n",
    "):\n",
    "    # ------------------------- STEP 2: Prepare data for visualization\n",
    "    data_iterator = iter(dataset_loader)\n",
    "    x_batch, y_batch = next(data_iterator)\n",
    "    while len(x_batch) < sample_count:\n",
    "        extra_x, extra_y = next(data_iterator)\n",
    "        x_batch = torch.cat((x_batch, extra_x), dim=0)\n",
    "        y_batch = torch.cat((y_batch, extra_y), dim=0)\n",
    "\n",
    "    # --------------------------------------- Substep 2.1: Perform prediction if a model is provided\n",
    "    if prediction_model is not None:\n",
    "        model_outputs = prediction_model(x_batch)\n",
    "        _, predictions = torch.max(model_outputs, 1)\n",
    "        true_labels = [category_names[label] for label in y_batch]\n",
    "        predicted_labels = [category_names[label] for label in predictions]\n",
    "\n",
    "    # mean_values = torch.tensor(image_transform.mean).view(3, 1, 1)\n",
    "    # std_values = torch.tensor(image_transform.std).view(3, 1, 1)\n",
    "    mean_values = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std_values = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "    image_width = x_batch[0].shape[1]\n",
    "    image_height = x_batch[0].shape[2]\n",
    "    grid_columns = 6\n",
    "    grid_rows = sample_count // grid_columns + (1 if sample_count % grid_columns else 0)\n",
    "    image_grid = Image.new(\n",
    "        \"RGB\", (image_width * grid_columns, image_height * grid_rows), color=\"white\"\n",
    "    )\n",
    "\n",
    "    # ------------------------- STEP 3: Visualize and save images\n",
    "    for i in range(sample_count):\n",
    "        image_data = (x_batch[i] * std_values + mean_values).numpy()\n",
    "        image_data = np.transpose(image_data, (1, 2, 0))\n",
    "        img = Image.fromarray((image_data * 255).astype(np.uint8))\n",
    "\n",
    "        # --------------------------------------- Substep 3.1: Annotate images with labels or predictions\n",
    "        if prediction_model is None:\n",
    "            label_list = [\n",
    "                category_names[idx] + \":\" + str(round(y_batch[i][idx].item(), 2))\n",
    "                for idx in y_batch[i].nonzero()\n",
    "            ]\n",
    "            img_title = f\"{', '.join(label_list)}\"\n",
    "        else:\n",
    "            img_title = f\"GT: {true_labels[i]}\\nPred: {predicted_labels[i]}\"\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.text((10, 10), img_title, fill=\"white\")\n",
    "\n",
    "        # --------------------------------------- Substep 3.2: Compile images into a grid\n",
    "        row_num, col_num = divmod(i, grid_columns)\n",
    "        image_grid.paste(img, (col_num * image_width, row_num * image_height))\n",
    "\n",
    "    # --------------------------------------- Substep 3.3: Save the grid image\n",
    "    if not os.getcwd().split(os.sep)[-1].startswith(\"task\"):\n",
    "        save_directory = \"task2/\" + save_directory\n",
    "    image_grid.save(save_directory)\n",
    "    print(f\"Image saved at {save_directory}\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "# Function for splitting the data into train, validation, and test sets\n",
    "# def split_data(data, labels, dev_ratio=0.8, val_ratio=0.1):\n",
    "\n",
    "#     num_samples = len(data)\n",
    "#     num_dev = int(num_samples * dev_ratio)\n",
    "#     num_val = int(num_dev * val_ratio)\n",
    "\n",
    "#     indices = torch.randperm(num_samples).tolist()\n",
    "#     dev_indices = indices[:num_dev]\n",
    "#     test_indices = indices[num_dev:]\n",
    "\n",
    "#     dev_data = data[dev_indices]\n",
    "#     dev_labels = labels[dev_indices]\n",
    "#     test_data = data[test_indices]\n",
    "#     test_labels = labels[test_indices]\n",
    "\n",
    "#     train_indices = dev_indices[:-num_val]\n",
    "#     val_indices = dev_indices[-num_val:]\n",
    "\n",
    "#     train_data = dev_data[:-num_val]\n",
    "#     train_labels = dev_labels[:-num_val]\n",
    "#     val_data = dev_data[-num_val:]\n",
    "#     val_labels = dev_labels[-num_val:]\n",
    "\n",
    "#     return train_data, train_labels, val_data, val_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "def perform_training(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    vision_model,\n",
    "    model_optimizer,\n",
    "    loss_function,\n",
    "    device,\n",
    "    num_classes,\n",
    "    num_epochs=20,\n",
    "    sampling_method=1,\n",
    "):\n",
    "\n",
    "    auroc = torchmetrics.AUROC(\n",
    "        task=\"multiclass\", num_classes=num_classes, average=\"macro\"\n",
    "    )\n",
    "    accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_auc = 0.0\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    vision_model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_index, (x_input, y_label) in enumerate(train_loader, 0):\n",
    "            x_input, y_label = x_input.to(device), y_label.to(device)\n",
    "\n",
    "            # MixUp augmentation\n",
    "            mixup = MixUp(sampling_method)\n",
    "            mixed_x, mixed_y = mixup(x_input, y_label)\n",
    "\n",
    "            model_optimizer.zero_grad()\n",
    "\n",
    "            model_output = vision_model(mixed_x)\n",
    "            batch_loss = loss_function(model_output, mixed_y)\n",
    "            batch_loss.backward()\n",
    "            model_optimizer.step()\n",
    "\n",
    "            total_loss += batch_loss.item()\n",
    "            # Print after each batch (optional)\n",
    "            print(\n",
    "                f\"Epoch: {epoch + 1}, Batch: {batch_index + 1}, Batch Loss: {batch_loss.item():.3f}\"\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch + 1}, Training Loss: {total_loss / len(train_loader):.3f}\"\n",
    "        )\n",
    "        vision_model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                val_output = vision_model(x_val)\n",
    "                val_loss += loss_function(val_output, y_val).item()\n",
    "                auroc.update(val_output, y_val)\n",
    "                accuracy.update(val_output.argmax(dim=1), y_val)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_auc = auroc.compute()\n",
    "        val_accuracy = accuracy.compute()\n",
    "\n",
    "        # Update the best validation metrics\n",
    "        best_val_loss = min(best_val_loss, val_loss)\n",
    "        best_val_auc = max(best_val_auc, val_auc)\n",
    "        best_val_accuracy = max(best_val_accuracy, val_accuracy)\n",
    "\n",
    "        # Update the best metrics if the current epoch has better performance\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "        # Print the validation metrics for the current epoch\n",
    "        print(\n",
    "            f\"Validation Loss: {val_loss:.3f}, \"\n",
    "            f\"Validation AUC-ROC: {val_auc:.3f}, \"\n",
    "            f\"Validation Accuracy: {val_accuracy:.3f}\"\n",
    "        )\n",
    "\n",
    "        auroc.reset()\n",
    "        accuracy.reset()\n",
    "\n",
    "        vision_model.train()\n",
    "\n",
    "    # Print the summary of the best performance\n",
    "    # print(f\"\\nBest Performance (Epoch {best_epoch}):\")\n",
    "    print(f\"Validation AUC-ROC: {best_val_auc:.3f}\")\n",
    "    print(f\"Validation Accuracy: {best_val_accuracy:.3f}\")\n",
    "\n",
    "    torch.save(vision_model.state_dict(), f\"model_{sampling_method}.pth\")\n",
    "    print(\"Saved trained model\")\n",
    "\n",
    "    return best_val_loss, best_val_auc, best_val_accuracy\n",
    "\n",
    "\n",
    "def test_model(test_loader, vision_model, loss_function, device, num_classes):\n",
    "    vision_model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_auroc = torchmetrics.AUROC(task=\"multiclass\", num_classes=num_classes)\n",
    "    test_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vision_model(images)\n",
    "            test_loss += loss_function(outputs, labels).item()\n",
    "            test_auroc.update(outputs, labels)\n",
    "            test_accuracy.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_auroc_score = test_auroc.compute()\n",
    "    test_accuracy_score = test_accuracy.compute()\n",
    "\n",
    "    return test_loss, test_auroc_score, test_accuracy_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    # Update the transform to include resizing\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),  # Resize the images to 224x224\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),  # from https://pytorch.org/vision/stable/models.html\n",
    "        ]\n",
    "    )\n",
    "    # ...\n",
    "\n",
    "    # Load CIFAR-10 train dataset and further split it into test and validation\n",
    "    dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    #  Split\n",
    "    train_set, test_set = torch.utils.data.random_split(\n",
    "        dataset, [0.8, 0.2], generator=GEN_SEED\n",
    "    )\n",
    "    train_set, val_set = torch.utils.data.random_split(\n",
    "        train_set, [0.9, 0.1], generator=GEN_SEED\n",
    "    )\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(train_set, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Print the number of batches in each data loader\n",
    "    print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "    print(f\"Number of batches in val_loader: {len(val_loader)}\")\n",
    "    print(f\"Number of batches in test_loader: {len(test_loader)}\")\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = 20\n",
    "\n",
    "    for sampling_method in [1, 2]:\n",
    "        # MixUp initialization\n",
    "        mixup = MixUp(mix_sampling_method=sampling_method)\n",
    "\n",
    "        # Model setup\n",
    "        model = vit_b_32(weights=ViT_B_32_Weights.DEFAULT)\n",
    "        model.heads = nn.Sequential(\n",
    "            nn.Linear(model.heads[0].in_features, 10)\n",
    "        )  # Adjusting for CIFAR-10\n",
    "        model.to(device)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_val_auc = 0.0\n",
    "        best_val_accuracy = 0.0\n",
    "\n",
    "        # Training\n",
    "        labels = torch.tensor(\n",
    "            [dataset.targets[i] for i in train_set.indices]\n",
    "        )  # Define labels for the training set\n",
    "\n",
    "        best_val_loss, best_val_auc, best_val_accuracy = perform_training(\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            device,\n",
    "            10,  # num_classes argument\n",
    "            num_epochs,\n",
    "            sampling_method,\n",
    "        )\n",
    "\n",
    "    # Testing\n",
    "    test_loss, test_auc, test_accuracy = test_model(\n",
    "        test_loader, model, criterion, device, 10\n",
    "    )\n",
    "\n",
    "    # Print the summary of loss values and metrics on the holdout test set\n",
    "    print(f\"\\nSampling Method: {sampling_method}\")\n",
    "    print(f\"Test Loss: {test_loss:.3f}\")\n",
    "    print(f\"Test AUC-ROC: {test_auc:.3f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "    # Compare the results with those obtained during development\n",
    "    print(f\"\\nComparison with Development Results:\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss:.3f}\")\n",
    "    print(f\"Best Validation AUC-ROC: {best_val_auc:.3f}\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_accuracy:.3f}\")\n",
    "\n",
    "    save_sample_images(\n",
    "        transform,\n",
    "        test_loader,\n",
    "        category_names=[\n",
    "            \"plane\",\n",
    "            \"car\",\n",
    "            \"bird\",\n",
    "            \"cat\",\n",
    "            \"deer\",\n",
    "            \"dog\",\n",
    "            \"frog\",\n",
    "            \"horse\",\n",
    "            \"ship\",\n",
    "            \"truck\",\n",
    "        ],\n",
    "        save_directory=f\"result_{sampling_method}.png\",\n",
    "        prediction_model=model,\n",
    "        sample_count=36,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testonasmalldataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Number of batches in train_loader: 3\n",
      "Number of batches in val_loader: 1\n",
      "Number of batches in test_loader: 3\n",
      "Epoch: 1, Batch: 1, Batch Loss: 2.302\n",
      "Epoch: 1, Batch: 2, Batch Loss: 2.371\n",
      "Epoch: 1, Batch: 3, Batch Loss: 2.341\n",
      "Epoch: 1, Training Loss: 2.338\n",
      "Validation Loss: 2.115, Validation AUC-ROC: 0.396, Validation Accuracy: 0.300\n",
      "Epoch: 2, Batch: 1, Batch Loss: 2.186\n",
      "Epoch: 2, Batch: 2, Batch Loss: 2.183\n",
      "Epoch: 2, Batch: 3, Batch Loss: 1.922\n",
      "Epoch: 2, Training Loss: 2.097\n",
      "Validation Loss: 1.917, Validation AUC-ROC: 0.437, Validation Accuracy: 0.500\n",
      "Validation AUC-ROC: 0.437\n",
      "Validation Accuracy: 0.500\n",
      "Saved trained model\n",
      "\n",
      "Sampling Method: 1\n",
      "Test Loss: 1.848\n",
      "Test AUC-ROC: 0.825\n",
      "Test Accuracy: 0.525\n",
      "\n",
      "Comparison with Development Results:\n",
      "Best Validation Loss: 1.917\n",
      "Best Validation AUC-ROC: 0.437\n",
      "Best Validation Accuracy: 0.500\n",
      "Image saved at result_1.png\n",
      "Epoch: 1, Batch: 1, Batch Loss: 2.352\n",
      "Epoch: 1, Batch: 2, Batch Loss: 2.308\n",
      "Epoch: 1, Batch: 3, Batch Loss: 2.225\n",
      "Epoch: 1, Training Loss: 2.295\n",
      "Validation Loss: 2.182, Validation AUC-ROC: 0.345, Validation Accuracy: 0.200\n",
      "Epoch: 2, Batch: 1, Batch Loss: 2.242\n",
      "Epoch: 2, Batch: 2, Batch Loss: 2.229\n",
      "Epoch: 2, Batch: 3, Batch Loss: 2.124\n",
      "Epoch: 2, Training Loss: 2.198\n",
      "Validation Loss: 2.157, Validation AUC-ROC: 0.365, Validation Accuracy: 0.100\n",
      "Validation AUC-ROC: 0.365\n",
      "Validation Accuracy: 0.200\n",
      "Saved trained model\n",
      "\n",
      "Sampling Method: 2\n",
      "Test Loss: 2.013\n",
      "Test AUC-ROC: 0.789\n",
      "Test Accuracy: 0.387\n",
      "\n",
      "Comparison with Development Results:\n",
      "Best Validation Loss: 2.157\n",
      "Best Validation AUC-ROC: 0.365\n",
      "Best Validation Accuracy: 0.200\n",
      "Image saved at result_2.png\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set up device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    # Update the transform to include resizing\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),  # Resize the images to 224x224\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),  # from https://pytorch.org/vision/stable/models.html\n",
    "        ]\n",
    "    )\n",
    "    # ...\n",
    "    # Load a subset of the CIFAR-10 dataset for testing\n",
    "    subset_size = 100  # Specify the desired size of the subset\n",
    "    dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    subset_indices = list(range(subset_size))\n",
    "    subset = torch.utils.data.Subset(dataset, subset_indices)\n",
    "\n",
    "    #  Split\n",
    "    train_set, test_set = torch.utils.data.random_split(\n",
    "        dataset, [0.8, 0.2], generator=GEN_SEED\n",
    "    )\n",
    "    train_set, val_set = torch.utils.data.random_split(\n",
    "        train_set, [0.9, 0.1], generator=GEN_SEED\n",
    "    )\n",
    "    # Split the subset into train, validation, and test sets\n",
    "    train_size = int(0.8 * subset_size)\n",
    "    val_size = int(0.1 * subset_size)\n",
    "    test_size = subset_size - train_size - val_size\n",
    "\n",
    "    train_set, test_set, val_set = torch.utils.data.random_split(\n",
    "        subset, [train_size, test_size, val_size], generator=GEN_SEED\n",
    "    )\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(train_set, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Print the number of batches in each data loader\n",
    "    print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "    print(f\"Number of batches in val_loader: {len(val_loader)}\")\n",
    "    print(f\"Number of batches in test_loader: {len(test_loader)}\")\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = 2\n",
    "\n",
    "    for sampling_method in [1, 2]:\n",
    "        # MixUp initialization\n",
    "        mixup = MixUp(mix_sampling_method=sampling_method)\n",
    "\n",
    "        # Model setup\n",
    "        model = vit_b_32(weights=ViT_B_32_Weights.DEFAULT)\n",
    "        model.heads = nn.Sequential(\n",
    "            nn.Linear(model.heads[0].in_features, 10)\n",
    "        )  # Adjusting for CIFAR-10\n",
    "        model.to(device)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_val_auc = 0.0\n",
    "        best_val_accuracy = 0.0\n",
    "\n",
    "        # Training\n",
    "        labels = torch.tensor(\n",
    "            [dataset.targets[i] for i in train_set.indices]\n",
    "        )  # Define labels for the training set\n",
    "\n",
    "        best_val_loss, best_val_auc, best_val_accuracy = perform_training(\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            device,\n",
    "            10,  # num_classes argument\n",
    "            num_epochs,\n",
    "            sampling_method,\n",
    "        )\n",
    "\n",
    "        # Testing\n",
    "        test_loss, test_auc, test_accuracy = test_model(\n",
    "            test_loader, model, criterion, device, 10\n",
    "        )\n",
    "\n",
    "        # Print the summary of loss values and metrics on the holdout test set\n",
    "        print(f\"\\nSampling Method: {sampling_method}\")\n",
    "        print(f\"Test Loss: {test_loss:.3f}\")\n",
    "        print(f\"Test AUC-ROC: {test_auc:.3f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "        # Compare the results with those obtained during development\n",
    "        print(f\"\\nComparison with Development Results:\")\n",
    "        print(f\"Best Validation Loss: {best_val_loss:.3f}\")\n",
    "        print(f\"Best Validation AUC-ROC: {best_val_auc:.3f}\")\n",
    "        print(f\"Best Validation Accuracy: {best_val_accuracy:.3f}\")\n",
    "\n",
    "        save_sample_images(\n",
    "            transform,\n",
    "            test_loader,\n",
    "            category_names=[\n",
    "                \"plane\",\n",
    "                \"car\",\n",
    "                \"bird\",\n",
    "                \"cat\",\n",
    "                \"deer.\",\n",
    "                \"dog\",\n",
    "                \"frog\",\n",
    "                \"horse\",\n",
    "                \"ship\",\n",
    "                \"truck\",\n",
    "            ],\n",
    "            save_directory=f\"result_{sampling_method}.png\",\n",
    "            prediction_model=model,\n",
    "            sample_count=36,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-cw1-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
