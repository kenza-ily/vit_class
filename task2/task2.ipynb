{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# ----------------- Importing Libraries  -------------------------------------\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, Normalize, Resize\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import matplotlib as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# ----------------- FIXED VALUES ------------------------------------\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MixUp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Define the transformation for CIFAR-10 dataset\n",
    "transform = ToTensor()\n",
    "\n",
    "# CIFAR-10 training dataset and DataLoader\n",
    "trainset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "class MixUp(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Mixup augmentation for training data.\n",
    "\n",
    "    Parameters:\n",
    "    sampling_method (int): Sampling method for mixup. 1: beta distribution, 2: uniform distribution\n",
    "    num_classes (int): Number of classes in the dataset\n",
    "    alpha (float): Alpha parameter for beta distribution\n",
    "    uniform_range (list): Range for uniform distribution\n",
    "\n",
    "    Returns:\n",
    "    augmented_images (torch.Tensor): Input data after mixup\n",
    "    augmented_labels (torch.Tensor): Target data after mixup\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_method: int = 1,\n",
    "        num_classes: int = 10,\n",
    "        alpha: float = 1.0,\n",
    "        uniform_range: list = [0.0, 1.0],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sampling_method = sampling_method\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.uniform_range = uniform_range\n",
    "\n",
    "    def __call__(self, images, labels):\n",
    "        # Set random seeds for reproducibility\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "        # Sample lambda for mixup\n",
    "        if self.sampling_method == 1:\n",
    "            mixup_lambda = np.random.beta(self.alpha, self.alpha)\n",
    "        elif self.sampling_method == 2:\n",
    "            mixup_lambda = np.random.uniform(\n",
    "                self.uniform_range[0], self.uniform_range[1]\n",
    "            )\n",
    "\n",
    "        # Perform mixup\n",
    "        shuffled_indices = torch.randperm(len(labels))\n",
    "        one_hot_labels = F.one_hot(labels, num_classes=self.num_classes).float()\n",
    "        augmented_images = (\n",
    "            mixup_lambda * images + (1 - mixup_lambda) * images[shuffled_indices, :]\n",
    "        )\n",
    "        augmented_labels = (\n",
    "            mixup_lambda * one_hot_labels\n",
    "            + (1 - mixup_lambda) * one_hot_labels[shuffled_indices]\n",
    "        )\n",
    "\n",
    "        return augmented_images, augmented_labels\n",
    "\n",
    "    def save_output(\n",
    "        self, images, labels, augmented_images, augmented_labels, save_path=\"mixup.png\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save the original and augmented images and labels as an image file.\n",
    "\n",
    "        Parameters:\n",
    "        images (torch.Tensor): Original input images\n",
    "        labels (torch.Tensor): Original input labels\n",
    "        augmented_images (torch.Tensor): Augmented input images\n",
    "        augmented_labels (torch.Tensor): Augmented input labels\n",
    "        save_path (str): Path to save the visualization image\n",
    "        \"\"\"\n",
    "        # Get the predicted class labels\n",
    "        _, predicted_labels = torch.max(augmented_labels, dim=1)\n",
    "        predicted_labels = predicted_labels.numpy()\n",
    "\n",
    "        # Create a grid of original and augmented images\n",
    "        grid_images = torch.cat([images, augmented_images], dim=0)\n",
    "        grid_labels = np.concatenate([labels.numpy(), predicted_labels], axis=0)\n",
    "\n",
    "        # Normalize the images to [0, 1] range\n",
    "        grid_images = (grid_images * 2.0) - 1.0\n",
    "\n",
    "        # Save the grid as an image file\n",
    "        grid_tensor = vutils.make_grid(\n",
    "            grid_images,\n",
    "            nrow=images.shape[0],\n",
    "            padding=2,\n",
    "            normalize=True,\n",
    "            value_range=(-1, 1),\n",
    "        )\n",
    "        vutils.save_image(grid_tensor, save_path)\n",
    "\n",
    "        # Save the labels to a text file\n",
    "        label_file = save_path.rsplit(\".\", 1)[0] + \"_labels.txt\"\n",
    "        with open(label_file, \"w\") as f:\n",
    "            for label in grid_labels:\n",
    "                f.write(str(label) + \"\\n\")\n",
    "\n",
    "\n",
    "# Create an instance of MixUp\n",
    "mixup = MixUp(sampling_method=1, num_classes=10, alpha=1.0, uniform_range=[0.0, 1.0])\n",
    "\n",
    "# Get the first 16 images and labels from the training set\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images[:10]\n",
    "labels = labels[:10]\n",
    "\n",
    "# Apply mixup augmentation\n",
    "augmented_images, augmented_labels = mixup(images, labels)\n",
    "\n",
    "# Save the output\n",
    "mixup.save_output(\n",
    "    images, labels, augmented_images, augmented_labels, save_path=\"mixup.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Define the transformation for CIFAR-10 dataset\n",
    "transform = ToTensor()\n",
    "\n",
    "# CIFAR-10 training dataset and DataLoader\n",
    "trainset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "class MixUp(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Mixup augmentation for training data.\n",
    "\n",
    "    Parameters:\n",
    "    sampling_method (int): Sampling method for mixup. 1: beta distribution, 2: uniform distribution\n",
    "    num_classes (int): Number of classes in the dataset\n",
    "    alpha (float): Alpha parameter for beta distribution\n",
    "    uniform_range (list): Range for uniform distribution\n",
    "\n",
    "    Returns:\n",
    "    augmented_images (torch.Tensor): Input data after mixup\n",
    "    augmented_labels (torch.Tensor): Target data after mixup\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_method: int = 1,\n",
    "        num_classes: int = 10,\n",
    "        alpha: float = 1.0,\n",
    "        uniform_range: list = [0.0, 1.0],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sampling_method = sampling_method\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.uniform_range = uniform_range\n",
    "\n",
    "    def __call__(self, images, labels):\n",
    "        # Set random seeds for reproducibility\n",
    "        np.random.seed(32)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Perform mixup for each image and label pair\n",
    "        shuffled_indices = torch.randperm(len(labels))\n",
    "        one_hot_labels = F.one_hot(labels, num_classes=self.num_classes).float()\n",
    "        augmented_images = []\n",
    "        augmented_labels = []\n",
    "        used_indices = set()\n",
    "\n",
    "        for img, label, idx in zip(images, one_hot_labels, shuffled_indices):\n",
    "            # Sample lambda for mixup\n",
    "            if self.sampling_method == 1:\n",
    "                mixup_lambda = np.random.beta(self.alpha, self.alpha)\n",
    "            elif self.sampling_method == 2:\n",
    "                mixup_lambda = np.random.uniform(\n",
    "                    self.uniform_range[0], self.uniform_range[1]\n",
    "                )\n",
    "\n",
    "            # Get a unique index for the second image\n",
    "            second_idx = idx\n",
    "            while second_idx in used_indices:\n",
    "                second_idx = torch.randperm(len(labels))[0]\n",
    "\n",
    "            augmented_img = (\n",
    "                mixup_lambda * img + (1 - mixup_lambda) * images[second_idx, :]\n",
    "            )\n",
    "            augmented_label = (\n",
    "                mixup_lambda * label + (1 - mixup_lambda) * one_hot_labels[second_idx]\n",
    "            )\n",
    "\n",
    "            augmented_images.append(augmented_img)\n",
    "            augmented_labels.append(augmented_label)\n",
    "            used_indices.add(second_idx)\n",
    "\n",
    "        augmented_images = torch.stack(augmented_images)\n",
    "        augmented_labels = torch.stack(augmented_labels)\n",
    "\n",
    "        return augmented_images, augmented_labels\n",
    "\n",
    "    # ... (the rest of the code remains the same) ...\n",
    "\n",
    "    def save_output(\n",
    "        self, images, labels, augmented_images, augmented_labels, save_path=\"mixup.png\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save the original and augmented images and labels as an image file.\n",
    "\n",
    "        Parameters:\n",
    "        images (torch.Tensor): Original input images\n",
    "        labels (torch.Tensor): Original input labels\n",
    "        augmented_images (torch.Tensor): Augmented input images\n",
    "        augmented_labels (torch.Tensor): Augmented input labels\n",
    "        save_path (str): Path to save the visualization image\n",
    "        \"\"\"\n",
    "        # Get the predicted class labels\n",
    "        _, predicted_labels = torch.max(augmented_labels, dim=1)\n",
    "        predicted_labels = predicted_labels.numpy()\n",
    "\n",
    "        # Create a grid of original and augmented images\n",
    "        grid_images = torch.cat([images, augmented_images], dim=0)\n",
    "        grid_labels = np.concatenate([labels.numpy(), predicted_labels], axis=0)\n",
    "\n",
    "        # Normalize the images to [0, 1] range\n",
    "        grid_images = (grid_images * 2.0) - 1.0\n",
    "\n",
    "        # Save the grid as an image file\n",
    "        grid_tensor = vutils.make_grid(\n",
    "            grid_images,\n",
    "            nrow=images.shape[0],\n",
    "            padding=2,\n",
    "            normalize=True,\n",
    "            value_range=(-1, 1),\n",
    "        )\n",
    "        vutils.save_image(grid_tensor, save_path)\n",
    "\n",
    "        # Save the labels to a text file\n",
    "        label_file = save_path.rsplit(\".\", 1)[0] + \"_labels.txt\"\n",
    "        with open(label_file, \"w\") as f:\n",
    "            for label in grid_labels:\n",
    "                f.write(str(label) + \"\\n\")\n",
    "\n",
    "\n",
    "# Create an instance of MixUp\n",
    "mixup = MixUp(sampling_method=1, num_classes=10, alpha=1.0, uniform_range=[0.0, 1.0])\n",
    "\n",
    "# Get the first 16 images and labels from the training set\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images[:10]\n",
    "labels = labels[:10]\n",
    "\n",
    "# Apply mixup augmentation\n",
    "augmented_images, augmented_labels = mixup(images, labels)\n",
    "\n",
    "# Save the output\n",
    "mixup.save_output(\n",
    "    images, labels, augmented_images, augmented_labels, save_path=\"mixup.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#  Randomly picked images\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.utils as vutils\n",
    "import random\n",
    "\n",
    "# Define the transformation for CIFAR-10 dataset\n",
    "transform = ToTensor()\n",
    "\n",
    "# CIFAR-10 training dataset and DataLoader\n",
    "trainset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "class MixUp(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Mixup augmentation for training data.\n",
    "\n",
    "    Parameters:\n",
    "    sampling_method (int): Sampling method for mixup. 1: beta distribution, 2: uniform distribution\n",
    "    num_classes (int): Number of classes in the dataset\n",
    "    alpha (float): Alpha parameter for beta distribution\n",
    "    uniform_range (list): Range for uniform distribution\n",
    "\n",
    "    Returns:\n",
    "    augmented_images (torch.Tensor): Input data after mixup\n",
    "    augmented_labels (torch.Tensor): Target data after mixup\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_method: int = 1,\n",
    "        num_classes: int = 10,\n",
    "        alpha: float = 1.0,\n",
    "        uniform_range: list = [0.0, 1.0],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sampling_method = sampling_method\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.uniform_range = uniform_range\n",
    "\n",
    "    def __call__(self, images, labels, num_ims=16):\n",
    "        # Set random seeds for reproducibility\n",
    "        np.random.seed(32)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Randomly select unique images from the dataset\n",
    "        all_indices = list(range(len(trainset)))\n",
    "        random.shuffle(all_indices)\n",
    "        selected_indices = all_indices[:num_ims]\n",
    "\n",
    "        # Perform mixup for the selected images\n",
    "        augmented_images = []\n",
    "        augmented_labels = []\n",
    "\n",
    "        for idx in selected_indices:\n",
    "            img, label = trainset[idx]\n",
    "            img = img.unsqueeze(0)  # Add batch dimension\n",
    "            label = torch.tensor([label])\n",
    "\n",
    "            # Sample lambda for mixup\n",
    "            if self.sampling_method == 1:\n",
    "                mixup_lambda = np.random.beta(self.alpha, self.alpha)\n",
    "            elif self.sampling_method == 2:\n",
    "                mixup_lambda = np.random.uniform(\n",
    "                    self.uniform_range[0], self.uniform_range[1]\n",
    "                )\n",
    "\n",
    "            # Get a unique index for the second image\n",
    "            second_idx = random.choice(all_indices)\n",
    "            while second_idx in selected_indices:\n",
    "                second_idx = random.choice(all_indices)\n",
    "\n",
    "            second_img, second_label = trainset[second_idx]\n",
    "            second_img = second_img.unsqueeze(0)\n",
    "            second_label = torch.tensor([second_label])\n",
    "\n",
    "            augmented_img = mixup_lambda * img + (1 - mixup_lambda) * second_img\n",
    "            augmented_label = (\n",
    "                mixup_lambda * F.one_hot(label, num_classes=self.num_classes).float()\n",
    "                + (1 - mixup_lambda)\n",
    "                * F.one_hot(second_label, num_classes=self.num_classes).float()\n",
    "            )\n",
    "\n",
    "            augmented_images.append(augmented_img)\n",
    "            augmented_labels.append(augmented_label)\n",
    "\n",
    "        augmented_images = torch.cat(augmented_images, dim=0)\n",
    "        augmented_labels = torch.cat(augmented_labels, dim=0)\n",
    "\n",
    "        return augmented_images, augmented_labels\n",
    "\n",
    "    def save_output(self, augmented_images, augmented_labels, save_path=\"mixup.png\"):\n",
    "        \"\"\"\n",
    "        Save the augmented images and labels as an image file.\n",
    "\n",
    "        Parameters:\n",
    "        augmented_images (torch.Tensor): Augmented input images\n",
    "        augmented_labels (torch.Tensor): Augmented input labels\n",
    "        save_path (str): Path to save the visualization image\n",
    "        \"\"\"\n",
    "        # Normalize the images to [-1, 1] range\n",
    "        augmented_images = (augmented_images * 2.0) - 1.0\n",
    "\n",
    "        # Save the grid as an image file\n",
    "        grid_tensor = vutils.make_grid(\n",
    "            augmented_images, nrow=4, padding=2, normalize=True, value_range=(-1, 1)\n",
    "        )\n",
    "        vutils.save_image(grid_tensor, save_path)\n",
    "\n",
    "        # Save the labels to a text file\n",
    "        label_file = save_path.rsplit(\".\", 1)[0] + \"_labels.txt\"\n",
    "        with open(label_file, \"w\") as f:\n",
    "            for label in augmented_labels:\n",
    "                f.write(str(torch.argmax(label).item()) + \"\\n\")\n",
    "\n",
    "\n",
    "# Create an instance of MixUp\n",
    "mixup = MixUp(sampling_method=1, num_classes=10, alpha=1.0, uniform_range=[0.0, 1.0])\n",
    "\n",
    "# Apply mixup augmentation\n",
    "augmented_images, augmented_labels = mixup(None, None, num_ims=16)\n",
    "\n",
    "# Save the output\n",
    "mixup.save_output(augmented_images, augmented_labels, save_path=\"mixup.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#  saving original images too\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.utils as vutils\n",
    "import random\n",
    "\n",
    "# Define the transformation for CIFAR-10 dataset\n",
    "transform = ToTensor()\n",
    "\n",
    "# CIFAR-10 training dataset and DataLoader\n",
    "trainset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "class MixUp(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Mixup augmentation for training data.\n",
    "\n",
    "    Parameters:\n",
    "    sampling_method (int): Sampling method for mixup. 1: beta distribution, 2: uniform distribution\n",
    "    num_classes (int): Number of classes in the dataset\n",
    "    alpha (float): Alpha parameter for beta distribution\n",
    "    uniform_range (list): Range for uniform distribution\n",
    "\n",
    "    Returns:\n",
    "    augmented_images (torch.Tensor): Input data after mixup\n",
    "    augmented_labels (torch.Tensor): Target data after mixup\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_method: int = 1,\n",
    "        num_classes: int = 10,\n",
    "        alpha: float = 1.0,\n",
    "        uniform_range: list = [0.0, 1.0],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sampling_method = sampling_method\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.uniform_range = uniform_range\n",
    "\n",
    "    def __call__(self, num_ims=16):\n",
    "        # Set random seeds for reproducibility\n",
    "        np.random.seed(32)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Randomly select unique images from the dataset\n",
    "        all_indices = list(range(len(trainset)))\n",
    "        random.shuffle(all_indices)\n",
    "        selected_indices = all_indices[:num_ims]\n",
    "\n",
    "        # Perform mixup for the selected images\n",
    "        augmented_images = []\n",
    "        augmented_labels = []\n",
    "        original_images = []\n",
    "\n",
    "        for idx in selected_indices:\n",
    "            img, label = trainset[idx]\n",
    "            img = img.unsqueeze(0)  # Add batch dimension\n",
    "            label = torch.tensor([label])\n",
    "            original_images.append(img)  # Store the original image\n",
    "\n",
    "            # Sample lambda for mixup\n",
    "            if self.sampling_method == 1:\n",
    "                mixup_lambda = np.random.beta(self.alpha, self.alpha)\n",
    "            elif self.sampling_method == 2:\n",
    "                mixup_lambda = np.random.uniform(\n",
    "                    self.uniform_range[0], self.uniform_range[1]\n",
    "                )\n",
    "\n",
    "            # Get a unique index for the second image\n",
    "            second_idx = random.choice(all_indices)\n",
    "            while second_idx in selected_indices:\n",
    "                second_idx = random.choice(all_indices)\n",
    "\n",
    "            second_img, second_label = trainset[second_idx]\n",
    "            second_img = second_img.unsqueeze(0)\n",
    "            second_label = torch.tensor([second_label])\n",
    "\n",
    "            augmented_img = mixup_lambda * img + (1 - mixup_lambda) * second_img\n",
    "            augmented_label = (\n",
    "                mixup_lambda * F.one_hot(label, num_classes=self.num_classes).float()\n",
    "                + (1 - mixup_lambda)\n",
    "                * F.one_hot(second_label, num_classes=self.num_classes).float()\n",
    "            )\n",
    "\n",
    "            augmented_images.append(augmented_img)\n",
    "            augmented_labels.append(augmented_label)\n",
    "\n",
    "        augmented_images = torch.cat(augmented_images, dim=0)\n",
    "        augmented_labels = torch.cat(augmented_labels, dim=0)\n",
    "        original_images = torch.cat(original_images, dim=0)\n",
    "\n",
    "        return original_images, augmented_images, augmented_labels\n",
    "\n",
    "    def save_output(\n",
    "        self, original_images, augmented_images, augmented_labels, save_path=\"mixup.png\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save the original, augmented images and labels as image files.\n",
    "\n",
    "        Parameters:\n",
    "        original_images (torch.Tensor): Original input images\n",
    "        augmented_images (torch.Tensor): Augmented input images\n",
    "        augmented_labels (torch.Tensor): Augmented input labels\n",
    "        save_path (str): Path to save the visualization image\n",
    "        \"\"\"\n",
    "        # Normalize the images to [-1, 1] range\n",
    "        original_images = (original_images * 2.0) - 1.0\n",
    "        augmented_images = (augmented_images * 2.0) - 1.0\n",
    "\n",
    "        # Save the original images\n",
    "        original_grid = vutils.make_grid(\n",
    "            original_images, nrow=4, padding=2, normalize=True, value_range=(-1, 1)\n",
    "        )\n",
    "        vutils.save_image(original_grid, \"original_mixup.png\")\n",
    "\n",
    "        # Save the augmented images\n",
    "        augmented_grid = vutils.make_grid(\n",
    "            augmented_images, nrow=4, padding=2, normalize=True, value_range=(-1, 1)\n",
    "        )\n",
    "        vutils.save_image(augmented_grid, save_path)\n",
    "\n",
    "        # Save the labels to a text file\n",
    "        label_file = save_path.rsplit(\".\", 1)[0] + \"_labels.txt\"\n",
    "        with open(label_file, \"w\") as f:\n",
    "            for label in augmented_labels:\n",
    "                f.write(str(torch.argmax(label).item()) + \"\\n\")\n",
    "\n",
    "\n",
    "# Create an instance of MixUp\n",
    "mixup = MixUp(sampling_method=1, num_classes=10, alpha=1.0, uniform_range=[0.0, 1.0])\n",
    "\n",
    "# Apply mixup augmentation\n",
    "original_images, augmented_images, augmented_labels = mixup(num_ims=16)\n",
    "\n",
    "# Save the output\n",
    "mixup.save_output(\n",
    "    original_images, augmented_images, augmented_labels, save_path=\"mixup.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision.models import vit_b_32\n",
    "\n",
    "\n",
    "# class ViT(nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super().__init__()\n",
    "#         self.vit = vit_b_32(pretrained=True)\n",
    "\n",
    "#         # Freeze all layers in the pretrained model\n",
    "#         for param in self.vit.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#         # Replace the head with a new linear layer\n",
    "#         self.vit.heads.head = nn.Linear(self.vit.heads.head.in_features, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.vit(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TRAINING CODE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_32\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.vit = vit_b_32(pretrained=True)\n",
    "\n",
    "        # Freeze all layers in the pretrained model\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace the head with a new linear layer\n",
    "        self.vit.heads.head = nn.Linear(\n",
    "            self.vit.heads.head.in_features, num_classes\n",
    "        )  # more efficient in terms of model size because it only replaces the final linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "# import torch.nn.utils.prune as prune\n",
    "# from torchvision.transforms import Resize\n",
    "# from models import ViT\n",
    "# from data import MixUp\n",
    "\n",
    "PRUNING_AMOUNT = 0.1\n",
    "\n",
    "\n",
    "def apply_pruning(module, amount=PRUNING_AMOUNT):\n",
    "    \"\"\"Apply unstructured pruning based on the L1 norm of weights.\"\"\"\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            prune.l1_unstructured(m, name=\"weight\", amount=amount)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "def train_with_mixup(sampling_method, num_epochs=20):\n",
    "\n",
    "    # Defining the data transformation for CIFAR-10\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
    "            transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "            transforms.Normalize(\n",
    "                (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "            ),  # Normalize the images\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load the CIFAR-10 dataset - train and test\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=\"data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=\"data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Define the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    # Ensure the SimplifiedViT class is correctly initialized as per your modifications\n",
    "    net = ViT().to(device)\n",
    "    net.vit.heads.head.apply(initialize_weights)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(\n",
    "        net.parameters(), lr=0.01, momentum=0.9\n",
    "    )  # v2 - lr=0.001 brought very low results with SimplifiedViT v1 -> lr=0.01\n",
    "    mixup = MixUp(alpha=1.0, sampling_method=sampling_method, seed=42)\n",
    "\n",
    "    # v2 - Introduce a learning rate scheduler\n",
    "    scheduler = lr_scheduler.StepLR(\n",
    "        optimizer, step_size=5, gamma=0.1\n",
    "    )  # Adjust learning rate every 5 epochs\n",
    "\n",
    "    train_acc, test_acc = [], []  # Initialize accuracy lists\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        # Training loop\n",
    "        net.train()  # Set the model to training mode\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs, targets_a, targets_b, lam = mixup(inputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (\n",
    "                (\n",
    "                    lam * (predicted == targets_a).float()\n",
    "                    + (1 - lam) * (predicted == targets_b).float()\n",
    "                )\n",
    "                .sum()\n",
    "                .item()\n",
    "            )\n",
    "\n",
    "        # v4 - Prunning\n",
    "        # Apply pruning at specified epochs and gradually increase the amount\n",
    "        if epoch % 5 == 4:  # Example: Apply pruning every 5 epochs\n",
    "            prune_amount = 0.05 + 0.05 * (\n",
    "                epoch // 5\n",
    "            )  # Increase pruning amount gradually\n",
    "            apply_pruning(net, amount=prune_amount)\n",
    "            print(f\"Applied pruning with amount {prune_amount:.2f}\")\n",
    "\n",
    "        # v2 - Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        train_acc.append(100 * correct / total)\n",
    "        print(f\"Epoch {epoch+1} - Training accuracy: {train_acc[-1]:.2f}%\")\n",
    "\n",
    "        # Test loop\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc.append(100 * correct / total)\n",
    "        print(f\"Epoch {epoch+1} - Test accuracy: {test_acc[-1]:.2f}%\")\n",
    "\n",
    "    # Save the trained model\n",
    "    model_path = os.path.join(\".\", f\"model_sampling_{sampling_method}.pth\")\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "    print(f\"Model with sampling method {sampling_method} saved to {model_path}\")\n",
    "\n",
    "    return train_acc, test_acc\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Training with sampling method 1 (beta distribution)\")\n",
    "#     train_acc_1, test_acc_1 = train_with_mixup(sampling_method=1)\n",
    "\n",
    "#     print(\"Training with sampling method 2 (uniform distribution)\")\n",
    "#     train_acc_2, test_acc_2 = train_with_mixup(sampling_method=2)\n",
    "\n",
    "#     # Report test set performance\n",
    "#     print(\"Test set performance for sampling method 1:\")\n",
    "#     for epoch, acc in enumerate(test_acc_1):\n",
    "#         print(f\"Epoch {epoch+1} - Test accuracy: {acc:.2f}%\")\n",
    "\n",
    "#     print(\"Test set performance for sampling method 2:\")\n",
    "#     for epoch, acc in enumerate(test_acc_2):\n",
    "#         print(f\"Epoch {epoch+1} - Test accuracy: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from models import ViT\n",
    "\n",
    "\n",
    "def visualize_results(model_path, testloader, classes, num_images=36):\n",
    "    # Load the trained model\n",
    "    net = ViT()\n",
    "    net.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "    net.eval()\n",
    "\n",
    "    # Get a batch of test images\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = next(dataiter)\n",
    "\n",
    "    # Make predictions on the test images\n",
    "    images = images.cuda()\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Create a montage of the test images with labels\n",
    "    montage = make_grid(images[:num_images], nrow=6, padding=2).cpu()\n",
    "    montage_image = transforms.ToPILImage()(montage)\n",
    "\n",
    "    # Add labels to the montage\n",
    "    draw = ImageDraw.Draw(montage_image)\n",
    "    font = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        x = i % 6 * montage_image.width // 6 + 5\n",
    "        y = i // 6 * montage_image.height // 6 + 5\n",
    "        label_text = f\"Truth: {classes[labels[i]]}\\nPredicted: {classes[predicted[i]]}\"\n",
    "        draw.text((x, y), label_text, font=font, fill=\"black\")\n",
    "\n",
    "    # Save the montage as \"result.png\"\n",
    "    result_path = os.path.join(os.path.dirname(model_path), \"result.png\")\n",
    "    montage_image.save(result_path)\n",
    "    print(f\"Montage of test images with labels saved to {result_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ViT:\n\tMissing key(s) in state_dict: \"vit.class_token\", \"vit.conv_proj.weight\", \"vit.conv_proj.bias\", \"vit.encoder.pos_embedding\", \"vit.encoder.layers.encoder_layer_0.ln_1.weight\", \"vit.encoder.layers.encoder_layer_0.ln_1.bias\", \"vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_0.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_0.ln_2.weight\", \"vit.encoder.layers.encoder_layer_0.ln_2.bias\", \"vit.encoder.layers.encoder_layer_0.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_0.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_0.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_0.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_1.ln_1.weight\", \"vit.encoder.layers.encoder_layer_1.ln_1.bias\", \"vit.encoder.layers.encoder_layer_1.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_1.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_1.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_1.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_1.ln_2.weight\", \"vit.encoder.layers.encoder_layer_1.ln_2.bias\", \"vit.encoder.layers.encoder_layer_1.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_1.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_1.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_1.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_2.ln_1.weight\", \"vit.encoder.layers.encoder_layer_2.ln_1.bias\", \"vit.encoder.layers.encoder_layer_2.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_2.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_2.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_2.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_2.ln_2.weight\", \"vit.encoder.layers.encoder_layer_2.ln_2.bias\", \"vit.encoder.layers.encoder_layer_2.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_2.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_2.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_2.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_3.ln_1.weight\", \"vit.encoder.layers.encoder_layer_3.ln_1.bias\", \"vit.encoder.layers.encoder_layer_3.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_3.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_3.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_3.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_3.ln_2.weight\", \"vit.encoder.layers.encoder_layer_3.ln_2.bias\", \"vit.encoder.layers.encoder_layer_3.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_3.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_3.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_3.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_4.ln_1.weight\", \"vit.encoder.layers.encoder_layer_4.ln_1.bias\", \"vit.encoder.layers.encoder_layer_4.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_4.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_4.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_4.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_4.ln_2.weight\", \"vit.encoder.layers.encoder_layer_4.ln_2.bias\", \"vit.encoder.layers.encoder_layer_4.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_4.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_4.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_4.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_5.ln_1.weight\", \"vit.encoder.layers.encoder_layer_5.ln_1.bias\", \"vit.encoder.layers.encoder_layer_5.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_5.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_5.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_5.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_5.ln_2.weight\", \"vit.encoder.layers.encoder_layer_5.ln_2.bias\", \"vit.encoder.layers.encoder_layer_5.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_5.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_5.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_5.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_6.ln_1.weight\", \"vit.encoder.layers.encoder_layer_6.ln_1.bias\", \"vit.encoder.layers.encoder_layer_6.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_6.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_6.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_6.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_6.ln_2.weight\", \"vit.encoder.layers.encoder_layer_6.ln_2.bias\", \"vit.encoder.layers.encoder_layer_6.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_6.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_6.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_6.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_7.ln_1.weight\", \"vit.encoder.layers.encoder_layer_7.ln_1.bias\", \"vit.encoder.layers.encoder_layer_7.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_7.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_7.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_7.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_7.ln_2.weight\", \"vit.encoder.layers.encoder_layer_7.ln_2.bias\", \"vit.encoder.layers.encoder_layer_7.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_7.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_7.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_7.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_8.ln_1.weight\", \"vit.encoder.layers.encoder_layer_8.ln_1.bias\", \"vit.encoder.layers.encoder_layer_8.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_8.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_8.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_8.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_8.ln_2.weight\", \"vit.encoder.layers.encoder_layer_8.ln_2.bias\", \"vit.encoder.layers.encoder_layer_8.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_8.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_8.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_8.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_9.ln_1.weight\", \"vit.encoder.layers.encoder_layer_9.ln_1.bias\", \"vit.encoder.layers.encoder_layer_9.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_9.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_9.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_9.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_9.ln_2.weight\", \"vit.encoder.layers.encoder_layer_9.ln_2.bias\", \"vit.encoder.layers.encoder_layer_9.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_9.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_9.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_9.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_10.ln_1.weight\", \"vit.encoder.layers.encoder_layer_10.ln_1.bias\", \"vit.encoder.layers.encoder_layer_10.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_10.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_10.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_10.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_10.ln_2.weight\", \"vit.encoder.layers.encoder_layer_10.ln_2.bias\", \"vit.encoder.layers.encoder_layer_10.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_10.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_10.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_10.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_11.ln_1.weight\", \"vit.encoder.layers.encoder_layer_11.ln_1.bias\", \"vit.encoder.layers.encoder_layer_11.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_11.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_11.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_11.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_11.ln_2.weight\", \"vit.encoder.layers.encoder_layer_11.ln_2.bias\", \"vit.encoder.layers.encoder_layer_11.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_11.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_11.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_11.mlp.3.bias\", \"vit.encoder.ln.weight\", \"vit.encoder.ln.bias\", \"vit.heads.head.weight\", \"vit.heads.head.bias\". \n\tUnexpected key(s) in state_dict: \"patch_embedding.position_embeddings\", \"patch_embedding.cls_token\", \"patch_embedding.projection.bias\", \"patch_embedding.projection.weight_orig\", \"patch_embedding.projection.weight_mask\", \"transformer_blocks.0.attention.keys.bias\", \"transformer_blocks.0.attention.keys.weight_orig\", \"transformer_blocks.0.attention.keys.weight_mask\", \"transformer_blocks.0.attention.queries.bias\", \"transformer_blocks.0.attention.queries.weight_orig\", \"transformer_blocks.0.attention.queries.weight_mask\", \"transformer_blocks.0.attention.values.bias\", \"transformer_blocks.0.attention.values.weight_orig\", \"transformer_blocks.0.attention.values.weight_mask\", \"transformer_blocks.0.attention.fc_out.bias\", \"transformer_blocks.0.attention.fc_out.weight_orig\", \"transformer_blocks.0.attention.fc_out.weight_mask\", \"transformer_blocks.0.norm1.weight\", \"transformer_blocks.0.norm1.bias\", \"transformer_blocks.0.norm2.weight\", \"transformer_blocks.0.norm2.bias\", \"transformer_blocks.0.feed_forward.0.bias\", \"transformer_blocks.0.feed_forward.0.weight_orig\", \"transformer_blocks.0.feed_forward.0.weight_mask\", \"transformer_blocks.0.feed_forward.2.bias\", \"transformer_blocks.0.feed_forward.2.weight_orig\", \"transformer_blocks.0.feed_forward.2.weight_mask\", \"transformer_blocks.1.attention.keys.bias\", \"transformer_blocks.1.attention.keys.weight_orig\", \"transformer_blocks.1.attention.keys.weight_mask\", \"transformer_blocks.1.attention.queries.bias\", \"transformer_blocks.1.attention.queries.weight_orig\", \"transformer_blocks.1.attention.queries.weight_mask\", \"transformer_blocks.1.attention.values.bias\", \"transformer_blocks.1.attention.values.weight_orig\", \"transformer_blocks.1.attention.values.weight_mask\", \"transformer_blocks.1.attention.fc_out.bias\", \"transformer_blocks.1.attention.fc_out.weight_orig\", \"transformer_blocks.1.attention.fc_out.weight_mask\", \"transformer_blocks.1.norm1.weight\", \"transformer_blocks.1.norm1.bias\", \"transformer_blocks.1.norm2.weight\", \"transformer_blocks.1.norm2.bias\", \"transformer_blocks.1.feed_forward.0.bias\", \"transformer_blocks.1.feed_forward.0.weight_orig\", \"transformer_blocks.1.feed_forward.0.weight_mask\", \"transformer_blocks.1.feed_forward.2.bias\", \"transformer_blocks.1.feed_forward.2.weight_orig\", \"transformer_blocks.1.feed_forward.2.weight_mask\", \"transformer_blocks.2.attention.keys.bias\", \"transformer_blocks.2.attention.keys.weight_orig\", \"transformer_blocks.2.attention.keys.weight_mask\", \"transformer_blocks.2.attention.queries.bias\", \"transformer_blocks.2.attention.queries.weight_orig\", \"transformer_blocks.2.attention.queries.weight_mask\", \"transformer_blocks.2.attention.values.bias\", \"transformer_blocks.2.attention.values.weight_orig\", \"transformer_blocks.2.attention.values.weight_mask\", \"transformer_blocks.2.attention.fc_out.bias\", \"transformer_blocks.2.attention.fc_out.weight_orig\", \"transformer_blocks.2.attention.fc_out.weight_mask\", \"transformer_blocks.2.norm1.weight\", \"transformer_blocks.2.norm1.bias\", \"transformer_blocks.2.norm2.weight\", \"transformer_blocks.2.norm2.bias\", \"transformer_blocks.2.feed_forward.0.bias\", \"transformer_blocks.2.feed_forward.0.weight_orig\", \"transformer_blocks.2.feed_forward.0.weight_mask\", \"transformer_blocks.2.feed_forward.2.bias\", \"transformer_blocks.2.feed_forward.2.weight_orig\", \"transformer_blocks.2.feed_forward.2.weight_mask\", \"classifier.bias\", \"classifier.weight_orig\", \"classifier.weight_mask\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Visualize the results for model_sampling_1.pth\u001b[39;00m\n\u001b[1;32m     38\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_sampling_1.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mvisualize_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Visualize the results for model_sampling_2.pth\u001b[39;00m\n\u001b[1;32m     42\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_sampling_2.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 12\u001b[0m, in \u001b[0;36mvisualize_results\u001b[0;34m(model_path, testloader, classes, num_images)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_results\u001b[39m(model_path, testloader, classes, num_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m36\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     net \u001b[38;5;241m=\u001b[39m ViT()\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Get a batch of test images\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ViT:\n\tMissing key(s) in state_dict: \"vit.class_token\", \"vit.conv_proj.weight\", \"vit.conv_proj.bias\", \"vit.encoder.pos_embedding\", \"vit.encoder.layers.encoder_layer_0.ln_1.weight\", \"vit.encoder.layers.encoder_layer_0.ln_1.bias\", \"vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_0.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_0.ln_2.weight\", \"vit.encoder.layers.encoder_layer_0.ln_2.bias\", \"vit.encoder.layers.encoder_layer_0.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_0.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_0.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_0.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_1.ln_1.weight\", \"vit.encoder.layers.encoder_layer_1.ln_1.bias\", \"vit.encoder.layers.encoder_layer_1.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_1.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_1.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_1.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_1.ln_2.weight\", \"vit.encoder.layers.encoder_layer_1.ln_2.bias\", \"vit.encoder.layers.encoder_layer_1.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_1.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_1.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_1.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_2.ln_1.weight\", \"vit.encoder.layers.encoder_layer_2.ln_1.bias\", \"vit.encoder.layers.encoder_layer_2.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_2.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_2.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_2.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_2.ln_2.weight\", \"vit.encoder.layers.encoder_layer_2.ln_2.bias\", \"vit.encoder.layers.encoder_layer_2.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_2.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_2.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_2.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_3.ln_1.weight\", \"vit.encoder.layers.encoder_layer_3.ln_1.bias\", \"vit.encoder.layers.encoder_layer_3.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_3.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_3.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_3.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_3.ln_2.weight\", \"vit.encoder.layers.encoder_layer_3.ln_2.bias\", \"vit.encoder.layers.encoder_layer_3.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_3.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_3.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_3.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_4.ln_1.weight\", \"vit.encoder.layers.encoder_layer_4.ln_1.bias\", \"vit.encoder.layers.encoder_layer_4.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_4.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_4.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_4.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_4.ln_2.weight\", \"vit.encoder.layers.encoder_layer_4.ln_2.bias\", \"vit.encoder.layers.encoder_layer_4.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_4.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_4.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_4.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_5.ln_1.weight\", \"vit.encoder.layers.encoder_layer_5.ln_1.bias\", \"vit.encoder.layers.encoder_layer_5.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_5.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_5.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_5.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_5.ln_2.weight\", \"vit.encoder.layers.encoder_layer_5.ln_2.bias\", \"vit.encoder.layers.encoder_layer_5.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_5.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_5.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_5.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_6.ln_1.weight\", \"vit.encoder.layers.encoder_layer_6.ln_1.bias\", \"vit.encoder.layers.encoder_layer_6.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_6.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_6.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_6.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_6.ln_2.weight\", \"vit.encoder.layers.encoder_layer_6.ln_2.bias\", \"vit.encoder.layers.encoder_layer_6.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_6.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_6.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_6.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_7.ln_1.weight\", \"vit.encoder.layers.encoder_layer_7.ln_1.bias\", \"vit.encoder.layers.encoder_layer_7.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_7.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_7.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_7.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_7.ln_2.weight\", \"vit.encoder.layers.encoder_layer_7.ln_2.bias\", \"vit.encoder.layers.encoder_layer_7.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_7.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_7.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_7.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_8.ln_1.weight\", \"vit.encoder.layers.encoder_layer_8.ln_1.bias\", \"vit.encoder.layers.encoder_layer_8.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_8.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_8.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_8.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_8.ln_2.weight\", \"vit.encoder.layers.encoder_layer_8.ln_2.bias\", \"vit.encoder.layers.encoder_layer_8.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_8.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_8.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_8.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_9.ln_1.weight\", \"vit.encoder.layers.encoder_layer_9.ln_1.bias\", \"vit.encoder.layers.encoder_layer_9.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_9.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_9.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_9.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_9.ln_2.weight\", \"vit.encoder.layers.encoder_layer_9.ln_2.bias\", \"vit.encoder.layers.encoder_layer_9.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_9.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_9.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_9.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_10.ln_1.weight\", \"vit.encoder.layers.encoder_layer_10.ln_1.bias\", \"vit.encoder.layers.encoder_layer_10.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_10.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_10.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_10.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_10.ln_2.weight\", \"vit.encoder.layers.encoder_layer_10.ln_2.bias\", \"vit.encoder.layers.encoder_layer_10.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_10.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_10.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_10.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_11.ln_1.weight\", \"vit.encoder.layers.encoder_layer_11.ln_1.bias\", \"vit.encoder.layers.encoder_layer_11.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_11.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_11.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_11.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_11.ln_2.weight\", \"vit.encoder.layers.encoder_layer_11.ln_2.bias\", \"vit.encoder.layers.encoder_layer_11.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_11.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_11.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_11.mlp.3.bias\", \"vit.encoder.ln.weight\", \"vit.encoder.ln.bias\", \"vit.heads.head.weight\", \"vit.heads.head.bias\". \n\tUnexpected key(s) in state_dict: \"patch_embedding.position_embeddings\", \"patch_embedding.cls_token\", \"patch_embedding.projection.bias\", \"patch_embedding.projection.weight_orig\", \"patch_embedding.projection.weight_mask\", \"transformer_blocks.0.attention.keys.bias\", \"transformer_blocks.0.attention.keys.weight_orig\", \"transformer_blocks.0.attention.keys.weight_mask\", \"transformer_blocks.0.attention.queries.bias\", \"transformer_blocks.0.attention.queries.weight_orig\", \"transformer_blocks.0.attention.queries.weight_mask\", \"transformer_blocks.0.attention.values.bias\", \"transformer_blocks.0.attention.values.weight_orig\", \"transformer_blocks.0.attention.values.weight_mask\", \"transformer_blocks.0.attention.fc_out.bias\", \"transformer_blocks.0.attention.fc_out.weight_orig\", \"transformer_blocks.0.attention.fc_out.weight_mask\", \"transformer_blocks.0.norm1.weight\", \"transformer_blocks.0.norm1.bias\", \"transformer_blocks.0.norm2.weight\", \"transformer_blocks.0.norm2.bias\", \"transformer_blocks.0.feed_forward.0.bias\", \"transformer_blocks.0.feed_forward.0.weight_orig\", \"transformer_blocks.0.feed_forward.0.weight_mask\", \"transformer_blocks.0.feed_forward.2.bias\", \"transformer_blocks.0.feed_forward.2.weight_orig\", \"transformer_blocks.0.feed_forward.2.weight_mask\", \"transformer_blocks.1.attention.keys.bias\", \"transformer_blocks.1.attention.keys.weight_orig\", \"transformer_blocks.1.attention.keys.weight_mask\", \"transformer_blocks.1.attention.queries.bias\", \"transformer_blocks.1.attention.queries.weight_orig\", \"transformer_blocks.1.attention.queries.weight_mask\", \"transformer_blocks.1.attention.values.bias\", \"transformer_blocks.1.attention.values.weight_orig\", \"transformer_blocks.1.attention.values.weight_mask\", \"transformer_blocks.1.attention.fc_out.bias\", \"transformer_blocks.1.attention.fc_out.weight_orig\", \"transformer_blocks.1.attention.fc_out.weight_mask\", \"transformer_blocks.1.norm1.weight\", \"transformer_blocks.1.norm1.bias\", \"transformer_blocks.1.norm2.weight\", \"transformer_blocks.1.norm2.bias\", \"transformer_blocks.1.feed_forward.0.bias\", \"transformer_blocks.1.feed_forward.0.weight_orig\", \"transformer_blocks.1.feed_forward.0.weight_mask\", \"transformer_blocks.1.feed_forward.2.bias\", \"transformer_blocks.1.feed_forward.2.weight_orig\", \"transformer_blocks.1.feed_forward.2.weight_mask\", \"transformer_blocks.2.attention.keys.bias\", \"transformer_blocks.2.attention.keys.weight_orig\", \"transformer_blocks.2.attention.keys.weight_mask\", \"transformer_blocks.2.attention.queries.bias\", \"transformer_blocks.2.attention.queries.weight_orig\", \"transformer_blocks.2.attention.queries.weight_mask\", \"transformer_blocks.2.attention.values.bias\", \"transformer_blocks.2.attention.values.weight_orig\", \"transformer_blocks.2.attention.values.weight_mask\", \"transformer_blocks.2.attention.fc_out.bias\", \"transformer_blocks.2.attention.fc_out.weight_orig\", \"transformer_blocks.2.attention.fc_out.weight_mask\", \"transformer_blocks.2.norm1.weight\", \"transformer_blocks.2.norm1.bias\", \"transformer_blocks.2.norm2.weight\", \"transformer_blocks.2.norm2.bias\", \"transformer_blocks.2.feed_forward.0.bias\", \"transformer_blocks.2.feed_forward.0.weight_orig\", \"transformer_blocks.2.feed_forward.0.weight_mask\", \"transformer_blocks.2.feed_forward.2.bias\", \"transformer_blocks.2.feed_forward.2.weight_orig\", \"transformer_blocks.2.feed_forward.2.weight_mask\", \"classifier.bias\", \"classifier.weight_orig\", \"classifier.weight_mask\". "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from models import ViT\n",
    "\n",
    "# Define the data transformation for CIFAR-10\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the images\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the CIFAR-10 dataset - test\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"data\", train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the classes for CIFAR-10\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")\n",
    "\n",
    "# Visualize the results for model_sampling_1.pth\n",
    "model_path = os.path.join(\".\", \"model_sampling_1.pth\")\n",
    "visualize_results(model_path, testloader, classes)\n",
    "\n",
    "# Visualize the results for model_sampling_2.pth\n",
    "model_path = os.path.join(\".\", \"model_sampling_2.pth\")\n",
    "visualize_results(model_path, testloader, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-cw1-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
