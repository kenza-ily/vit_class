{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# ----------------- Importing Libraries  -------------------------------------\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, Normalize, Resize\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "import random\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "# ----------------- FIXED VALUES ------------------------------------\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Define the classes for CIFAR-10\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MixUp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#  saving original images too\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.utils as vutils\n",
    "import random\n",
    "\n",
    "# Define the transformation for CIFAR-10 dataset\n",
    "transform = ToTensor()\n",
    "\n",
    "# CIFAR-10 training dataset and DataLoader\n",
    "trainset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "class MixUp(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Mixup augmentation for training data.\n",
    "\n",
    "    Parameters:\n",
    "    sampling_method (int): Sampling method for mixup. 1: beta distribution, 2: uniform distribution\n",
    "    num_classes (int): Number of classes in the dataset\n",
    "    alpha (float): Alpha parameter for beta distribution\n",
    "    uniform_range (list): Range for uniform distribution\n",
    "\n",
    "    Returns:\n",
    "    augmented_images (torch.Tensor): Input data after mixup\n",
    "    augmented_labels (torch.Tensor): Target data after mixup\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_method: int = 1,\n",
    "        num_classes: int = 10,\n",
    "        alpha: float = 1.0,\n",
    "        uniform_range: list = [0.0, 1.0],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sampling_method = sampling_method\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.uniform_range = uniform_range\n",
    "\n",
    "    def __call__(self, num_ims=16):\n",
    "        # Set random seeds for reproducibility\n",
    "        np.random.seed(32)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Randomly select unique images from the dataset\n",
    "        all_indices = list(range(len(trainset)))\n",
    "        random.shuffle(all_indices)\n",
    "        selected_indices = all_indices[:num_ims]\n",
    "\n",
    "        # Perform mixup for the selected images\n",
    "        augmented_images = []\n",
    "        augmented_labels = []\n",
    "        original_images = []\n",
    "\n",
    "        for idx in selected_indices:\n",
    "            img, label = trainset[idx]\n",
    "            img = img.unsqueeze(0)  # Add batch dimension\n",
    "            label = torch.tensor([label])\n",
    "            original_images.append(img)  # Store the original image\n",
    "\n",
    "            # Sample lambda for mixup\n",
    "            if self.sampling_method == 1:\n",
    "                mixup_lambda = np.random.beta(self.alpha, self.alpha)\n",
    "            elif self.sampling_method == 2:\n",
    "                mixup_lambda = np.random.uniform(\n",
    "                    self.uniform_range[0], self.uniform_range[1]\n",
    "                )\n",
    "\n",
    "            # Get a unique index for the second image\n",
    "            second_idx = random.choice(all_indices)\n",
    "            while second_idx in selected_indices:\n",
    "                second_idx = random.choice(all_indices)\n",
    "\n",
    "            second_img, second_label = trainset[second_idx]\n",
    "            second_img = second_img.unsqueeze(0)\n",
    "            second_label = torch.tensor([second_label])\n",
    "\n",
    "            augmented_img = mixup_lambda * img + (1 - mixup_lambda) * second_img\n",
    "            augmented_label = (\n",
    "                mixup_lambda * F.one_hot(label, num_classes=self.num_classes).float()\n",
    "                + (1 - mixup_lambda)\n",
    "                * F.one_hot(second_label, num_classes=self.num_classes).float()\n",
    "            )\n",
    "\n",
    "            augmented_images.append(augmented_img)\n",
    "            augmented_labels.append(augmented_label)\n",
    "\n",
    "        augmented_images = torch.cat(augmented_images, dim=0)\n",
    "        augmented_labels = torch.cat(augmented_labels, dim=0)\n",
    "        original_images = torch.cat(original_images, dim=0)\n",
    "\n",
    "        # Assuming augmented_labels is a tuple containing two tensors: (labels_a, labels_b)\n",
    "        # augmented_labels_a, augmented_labels_b = augmented_labels\n",
    "\n",
    "        return original_images, augmented_images, augmented_labels\n",
    "\n",
    "    def save_output(\n",
    "        self, original_images, augmented_images, augmented_labels, save_path=\"mixup.png\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save the original, augmented images and labels as image files.\n",
    "\n",
    "        Parameters:\n",
    "        original_images (torch.Tensor): Original input images\n",
    "        augmented_images (torch.Tensor): Augmented input images\n",
    "        augmented_labels (torch.Tensor): Augmented input labels\n",
    "        save_path (str): Path to save the visualization image\n",
    "        \"\"\"\n",
    "        # Normalize the images to [-1, 1] range\n",
    "        original_images = (original_images * 2.0) - 1.0\n",
    "        augmented_images = (augmented_images * 2.0) - 1.0\n",
    "\n",
    "        # Save the original images\n",
    "        original_grid = vutils.make_grid(\n",
    "            original_images, nrow=4, padding=2, normalize=True, value_range=(-1, 1)\n",
    "        )\n",
    "        vutils.save_image(original_grid, \"original_mixup.png\")\n",
    "\n",
    "        # Save the augmented images\n",
    "        augmented_grid = vutils.make_grid(\n",
    "            augmented_images, nrow=4, padding=2, normalize=True, value_range=(-1, 1)\n",
    "        )\n",
    "        vutils.save_image(augmented_grid, save_path)\n",
    "\n",
    "        # Save the labels to a text file\n",
    "        label_file = save_path.rsplit(\".\", 1)[0] + \"_labels.txt\"\n",
    "        with open(label_file, \"w\") as f:\n",
    "            for label in augmented_labels:\n",
    "                f.write(str(torch.argmax(label).item()) + \"\\n\")\n",
    "\n",
    "\n",
    "# Create an instance of MixUp\n",
    "mixup = MixUp(sampling_method=1, num_classes=10, alpha=1.0, uniform_range=[0.0, 1.0])\n",
    "\n",
    "# Apply mixup augmentation\n",
    "original_images, augmented_images, augmented_labels = mixup(num_ims=16)\n",
    "\n",
    "# Save the output\n",
    "mixup.save_output(\n",
    "    original_images, augmented_images, augmented_labels, save_path=\"mixup.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixup with returned values\n",
    "#  saving original images too\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.utils as vutils\n",
    "import random\n",
    "\n",
    "# Define the transformation for CIFAR-10 dataset\n",
    "transform = ToTensor()\n",
    "\n",
    "# CIFAR-10 training dataset and DataLoader\n",
    "trainset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "class MixUp(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Mixup augmentation for training data.\n",
    "\n",
    "    Parameters:\n",
    "    sampling_method (int): Sampling method for mixup. 1: beta distribution, 2: uniform distribution\n",
    "    num_classes (int): Number of classes in the dataset\n",
    "    alpha (float): Alpha parameter for beta distribution\n",
    "    uniform_range (list): Range for uniform distribution\n",
    "\n",
    "    Returns:\n",
    "    augmented_images (torch.Tensor): Input data after mixup\n",
    "    augmented_labels (torch.Tensor): Target data after mixup\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_method: int = 1,\n",
    "        num_classes: int = 10,\n",
    "        alpha: float = 1.0,\n",
    "        uniform_range: list = [0.0, 1.0],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sampling_method = sampling_method\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.uniform_range = uniform_range\n",
    "\n",
    "    def __call__(self, num_ims=16):\n",
    "        # Set random seeds for reproducibility\n",
    "        np.random.seed(32)\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # Randomly select unique images from the dataset\n",
    "        all_indices = list(range(len(trainset)))\n",
    "        random.shuffle(all_indices)\n",
    "        selected_indices = all_indices[:num_ims]\n",
    "\n",
    "        # Perform mixup for the selected images\n",
    "        augmented_images = []\n",
    "        augmented_labels = []\n",
    "        original_images = []\n",
    "\n",
    "        for idx in selected_indices:\n",
    "            img, label = trainset[idx]\n",
    "            img = img.unsqueeze(0)  # Add batch dimension\n",
    "            label = torch.tensor([label])\n",
    "            original_images.append(img)  # Store the original image\n",
    "\n",
    "            # Sample lambda for mixup\n",
    "            if self.sampling_method == 1:\n",
    "                mixup_lambda = np.random.beta(self.alpha, self.alpha)\n",
    "            elif self.sampling_method == 2:\n",
    "                mixup_lambda = np.random.uniform(\n",
    "                    self.uniform_range[0], self.uniform_range[1]\n",
    "                )\n",
    "\n",
    "            # Get a unique index for the second image\n",
    "            second_idx = random.choice(all_indices)\n",
    "            while second_idx in selected_indices:\n",
    "                second_idx = random.choice(all_indices)\n",
    "\n",
    "            second_img, second_label = trainset[second_idx]\n",
    "            second_img = second_img.unsqueeze(0)\n",
    "            second_label = torch.tensor([second_label])\n",
    "\n",
    "            augmented_img = mixup_lambda * img + (1 - mixup_lambda) * second_img\n",
    "            augmented_label = (\n",
    "                mixup_lambda * F.one_hot(label, num_classes=self.num_classes).float()\n",
    "                + (1 - mixup_lambda)\n",
    "                * F.one_hot(second_label, num_classes=self.num_classes).float()\n",
    "            )\n",
    "\n",
    "            augmented_images.append(augmented_img)\n",
    "            augmented_labels.append(augmented_label)\n",
    "\n",
    "        augmented_images = torch.cat(augmented_images, dim=0)\n",
    "        augmented_labels = torch.cat(augmented_labels, dim=0)\n",
    "        original_images = torch.cat(original_images, dim=0)\n",
    "\n",
    "        # Assuming augmented_labels is a tuple containing two tensors: (labels_a, labels_b)\n",
    "        # augmented_labels_a, augmented_labels_b = augmented_labels\n",
    "\n",
    "        return original_images, augmented_images, augmented_labels\n",
    "\n",
    "    def save_output(\n",
    "        self, original_images, augmented_images, augmented_labels, save_path=\"mixup.png\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save the original, augmented images and labels as image files.\n",
    "\n",
    "        Parameters:\n",
    "        original_images (torch.Tensor): Original input images\n",
    "        augmented_images (torch.Tensor): Augmented input images\n",
    "        augmented_labels (torch.Tensor): Augmented input labels\n",
    "        save_path (str): Path to save the visualization image\n",
    "        \"\"\"\n",
    "        # Normalize the images to [-1, 1] range\n",
    "        original_images = (original_images * 2.0) - 1.0\n",
    "        augmented_images = (augmented_images * 2.0) - 1.0\n",
    "\n",
    "        # Save the original images\n",
    "        original_grid = vutils.make_grid(\n",
    "            original_images, nrow=4, padding=2, normalize=True, value_range=(-1, 1)\n",
    "        )\n",
    "        vutils.save_image(original_grid, \"original_mixup.png\")\n",
    "\n",
    "        # Save the augmented images\n",
    "        augmented_grid = vutils.make_grid(\n",
    "            augmented_images, nrow=4, padding=2, normalize=True, value_range=(-1, 1)\n",
    "        )\n",
    "        vutils.save_image(augmented_grid, save_path)\n",
    "\n",
    "        # Save the labels to a text file\n",
    "        label_file = save_path.rsplit(\".\", 1)[0] + \"_labels.txt\"\n",
    "        with open(label_file, \"w\") as f:\n",
    "            for label in augmented_labels:\n",
    "                f.write(str(torch.argmax(label).item()) + \"\\n\")\n",
    "\n",
    "\n",
    "# Create an instance of MixUp\n",
    "mixup = MixUp(sampling_method=1, num_classes=10, alpha=1.0, uniform_range=[0.0, 1.0])\n",
    "\n",
    "# Apply mixup augmentation\n",
    "original_images, augmented_images, augmented_labels = mixup(num_ims=16)\n",
    "\n",
    "# Save the output\n",
    "mixup.save_output(\n",
    "    original_images, augmented_images, augmented_labels, save_path=\"mixup.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision.models import vit_b_32\n",
    "\n",
    "\n",
    "# class ViT(nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super().__init__()\n",
    "#         self.vit = vit_b_32(pretrained=True)\n",
    "\n",
    "#         # Freeze all layers in the pretrained model\n",
    "#         for param in self.vit.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#         # Replace the head with a new linear layer\n",
    "#         self.vit.heads.head = nn.Linear(self.vit.heads.head.in_features, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.vit(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TRAINING CODE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_32\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.vit = vit_b_32(pretrained=True)\n",
    "\n",
    "        # Freeze all layers in the pretrained model\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace the head with a new linear layer\n",
    "        self.vit.heads.head = nn.Linear(\n",
    "            self.vit.heads.head.in_features, num_classes\n",
    "        )  # more efficient in terms of model size because it only replaces the final linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with sampling method 1 (beta distribution)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataLoader.__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 159\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with sampling method 1 (beta distribution)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m     train_acc_1, test_acc_1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_mixup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with sampling method 2 (uniform distribution)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m     train_acc_2, test_acc_2 \u001b[38;5;241m=\u001b[39m train_with_mixup(sampling_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 62\u001b[0m, in \u001b[0;36mtrain_with_mixup\u001b[0;34m(sampling_method, num_epochs)\u001b[0m\n\u001b[1;32m     57\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(trainset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m testset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10(\n\u001b[1;32m     60\u001b[0m     root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[1;32m     61\u001b[0m )\n\u001b[0;32m---> 62\u001b[0m testloader \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Define the device\u001b[39;00m\n\u001b[1;32m     65\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: DataLoader.__init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "# import torch.nn.utils.prune as prune\n",
    "# from torchvision.transforms import Resize\n",
    "# from models import ViT\n",
    "# from data import MixUp\n",
    "\n",
    "# PRUNING_AMOUNT = 0.1\n",
    "\n",
    "\n",
    "# def apply_pruning(module, amount=PRUNING_AMOUNT):\n",
    "#     \"\"\"Apply unstructured pruning based on the L1 norm of weights.\"\"\"\n",
    "#     for m in module.modules():\n",
    "#         if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "#             prune.l1_unstructured(m, name=\"weight\", amount=amount)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "def train_with_mixup(sampling_method, num_epochs=20):\n",
    "    # Defining the data transformation for CIFAR-10\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
    "            transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "            transforms.Normalize(\n",
    "                (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "            ),  # Normalize the images\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load the CIFAR-10 dataset - train and test\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=\"data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=\"data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Define the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    net = ViT().to(device)\n",
    "    net.vit.heads.head.apply(initialize_weights)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "    mixup = MixUp(alpha=1.0, sampling_method=sampling_method)\n",
    "\n",
    "    # v2 - Introduce a learning rate scheduler\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    train_acc, test_acc = [], []  # Initialize accuracy lists\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        # Training loop\n",
    "        net.train()  # Set the model to training mode\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Perform MixUp augmentation\n",
    "            original_images, augmented_images, augmented_labels = mixup(\n",
    "                num_ims=inputs.size(0)\n",
    "            )\n",
    "            augmented_images = augmented_images.to(device)\n",
    "            # Note: augmented_labels is used directly, no unpacking needed\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(augmented_images)\n",
    "\n",
    "            # Calculate loss using the entire augmented_labels tensor\n",
    "            # The loss function needs to be compatible with soft labels\n",
    "            loss = soft_label_criterion(outputs, augmented_labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (\n",
    "                (\n",
    "                    lam * (predicted == augmented_labels_a).float()\n",
    "                    + (1 - lam) * (predicted == augmented_labels_b).float()\n",
    "                )\n",
    "                .sum()\n",
    "                .item()\n",
    "            )\n",
    "\n",
    "        # v4 - Pruning\n",
    "        # Apply pruning at specified epochs and gradually increase the amount\n",
    "        # if epoch % 5 == 4:  # Example: Apply pruning every 5 epochs\n",
    "        #     prune_amount = 0.05 + 0.05 * (\n",
    "        #         epoch // 5\n",
    "        #     )  # Increase pruning amount gradually\n",
    "        #     apply_pruning(net, amount=prune_amount)\n",
    "        #     print(f\"Applied pruning with amount {prune_amount:.2f}\")\n",
    "\n",
    "        # v2 - Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        train_acc.append(100 * correct / total)\n",
    "        print(f\"Epoch {epoch+1} - Training accuracy: {train_acc[-1]:.2f}%\")\n",
    "\n",
    "        # Test loop\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc.append(100 * correct / total)\n",
    "        print(f\"Epoch {epoch+1} - Test accuracy: {test_acc[-1]:.2f}%\")\n",
    "\n",
    "    # Save the trained model\n",
    "    model_path = os.path.join(\".\", f\"model_sampling_{sampling_method}.pth\")\n",
    "    torch.save(net.state_dict(), model_path)\n",
    "    print(f\"Model with sampling method {sampling_method} saved to {model_path}\")\n",
    "\n",
    "    return train_acc, test_acc\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Training with sampling method 1 (beta distribution)\")\n",
    "    train_acc_1, test_acc_1 = train_with_mixup(sampling_method=1)\n",
    "\n",
    "    print(\"Training with sampling method 2 (uniform distribution)\")\n",
    "    train_acc_2, test_acc_2 = train_with_mixup(sampling_method=2)\n",
    "\n",
    "    # Report test set performance\n",
    "    print(\"Test set performance for sampling method 1:\")\n",
    "    for epoch, acc in enumerate(test_acc_1):\n",
    "        print(f\"Epoch {epoch+1} - Test accuracy: {acc:.2f}%\")\n",
    "\n",
    "    print(\"Test set performance for sampling method 2:\")\n",
    "    for epoch, acc in enumerate(test_acc_2):\n",
    "        print(f\"Epoch {epoch+1} - Test accuracy: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  to do augmentation and THEN postprocessing to have ViT entry\n",
    "\n",
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "# import torch.nn.utils.prune as prune\n",
    "# from torchvision.transforms import Resize\n",
    "# from models import ViT\n",
    "# from data import MixUp\n",
    "\n",
    "# PRUNING_AMOUNT = 0.1\n",
    "\n",
    "\n",
    "# def apply_pruning(module, amount=PRUNING_AMOUNT):\n",
    "#     \"\"\"Apply unstructured pruning based on the L1 norm of weights.\"\"\"\n",
    "#     for m in module.modules():\n",
    "#         if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "#             prune.l1_unstructured(m, name=\"weight\", amount=amount)\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from models import (\n",
    "    ViT,\n",
    ")  # Ensure this is the correct import for your Vision Transformer model\n",
    "from data import (\n",
    "    MixUp,\n",
    ")  # Ensure this is the correct import for your MixUp implementation\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from models import (\n",
    "    ViT,\n",
    ")  # Ensure this is the correct import for your Vision Transformer model\n",
    "from data import (\n",
    "    MixUp,\n",
    ")  # Ensure this is the correct import for your MixUp implementation\n",
    "\n",
    "\n",
    "def train_with_mixup(sampling_method, num_epochs=20):\n",
    "    # Simple transformation: Convert images to tensors and normalize\n",
    "    simple_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load CIFAR-10 dataset\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=simple_transform\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=simple_transform\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    net = ViT().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "    mixup = MixUp(alpha=1.0, sampling_method=sampling_method)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Apply MixUp augmentation\n",
    "            _, augmented_images, augmented_labels = mixup(inputs, labels)\n",
    "            augmented_images = (\n",
    "                augmented_images.cpu()\n",
    "            )  # Move to CPU for PIL transformation\n",
    "\n",
    "            # Manually apply resizing and normalization\n",
    "            resize_transform = transforms.Compose(\n",
    "                [\n",
    "                    ToPILImage(),\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                ]\n",
    "            )\n",
    "            transformed_images = torch.stack(\n",
    "                [resize_transform(img) for img in augmented_images]\n",
    "            ).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(transformed_images)\n",
    "            loss = criterion(outputs, augmented_labels.max(dim=1)[1])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == augmented_labels.max(dim=1)[1]).sum().item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} - Training Loss: {running_loss / total:.4f}, Training Accuracy: {100 * correct / total:.2f}%\"\n",
    "        )\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "    # Save the trained model\n",
    "    model_save_path = (\n",
    "        f\"./trained_models/model_sampling_{sampling_method}_epochs_{num_epochs}.pth\"\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(net.state_dict(), model_save_path)\n",
    "    print(f\"Model with sampling method {sampling_method} saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with sampling method 1 (e.g., beta distribution)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Training and saving the model with the first sampling method\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with sampling method 1 (e.g., beta distribution)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_with_mixup\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Training and saving the model with the second sampling method\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with sampling method 2 (e.g., uniform distribution)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 109\u001b[0m, in \u001b[0;36mtrain_with_mixup\u001b[0;34m(sampling_method, num_epochs)\u001b[0m\n\u001b[1;32m    106\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Apply MixUp augmentation\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m _, augmented_images, augmented_labels \u001b[38;5;241m=\u001b[39m mixup(inputs, labels)\n\u001b[1;32m    110\u001b[0m augmented_images \u001b[38;5;241m=\u001b[39m augmented_images\u001b[38;5;241m.\u001b[39mcpu()  \u001b[38;5;66;03m# Move to CPU for PIL transformation\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Manually apply resizing and normalization\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    num_epochs = 20  # Specify the number of epochs\n",
    "\n",
    "    # Training and saving the model with the first sampling method\n",
    "    print(\"Training with sampling method 1 (e.g., beta distribution)\")\n",
    "    train_with_mixup(sampling_method=1, num_epochs=num_epochs)\n",
    "\n",
    "    # Training and saving the model with the second sampling method\n",
    "    print(\"Training with sampling method 2 (e.g., uniform distribution)\")\n",
    "    train_with_mixup(sampling_method=2, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from models import ViT\n",
    "\n",
    "\n",
    "def visualize_results(model_path, testloader, classes, num_images=36):\n",
    "    # Load the trained model\n",
    "    net = ViT()\n",
    "    net.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "    net.eval()\n",
    "\n",
    "    # Get a batch of test images\n",
    "    dataiter = iter(testloader)\n",
    "    images, labels = next(dataiter)\n",
    "\n",
    "    # Make predictions on the test images\n",
    "    images = images.cuda()\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Create a montage of the test images with labels\n",
    "    montage = make_grid(images[:num_images], nrow=6, padding=2).cpu()\n",
    "    montage_image = transforms.ToPILImage()(montage)\n",
    "\n",
    "    # Add labels to the montage\n",
    "    draw = ImageDraw.Draw(montage_image)\n",
    "    font = ImageFont.truetype(\"arial.ttf\", 12)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        x = i % 6 * montage_image.width // 6 + 5\n",
    "        y = i // 6 * montage_image.height // 6 + 5\n",
    "        label_text = f\"Truth: {classes[labels[i]]}\\nPredicted: {classes[predicted[i]]}\"\n",
    "        draw.text((x, y), label_text, font=font, fill=\"black\")\n",
    "\n",
    "    # Save the montage as \"result.png\"\n",
    "    result_path = os.path.join(os.path.dirname(model_path), \"result.png\")\n",
    "    montage_image.save(result_path)\n",
    "    print(f\"Montage of test images with labels saved to {result_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kenzabenkirane/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_32_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_32_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ViT:\n\tMissing key(s) in state_dict: \"vit.class_token\", \"vit.conv_proj.weight\", \"vit.conv_proj.bias\", \"vit.encoder.pos_embedding\", \"vit.encoder.layers.encoder_layer_0.ln_1.weight\", \"vit.encoder.layers.encoder_layer_0.ln_1.bias\", \"vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_0.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_0.ln_2.weight\", \"vit.encoder.layers.encoder_layer_0.ln_2.bias\", \"vit.encoder.layers.encoder_layer_0.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_0.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_0.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_0.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_1.ln_1.weight\", \"vit.encoder.layers.encoder_layer_1.ln_1.bias\", \"vit.encoder.layers.encoder_layer_1.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_1.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_1.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_1.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_1.ln_2.weight\", \"vit.encoder.layers.encoder_layer_1.ln_2.bias\", \"vit.encoder.layers.encoder_layer_1.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_1.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_1.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_1.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_2.ln_1.weight\", \"vit.encoder.layers.encoder_layer_2.ln_1.bias\", \"vit.encoder.layers.encoder_layer_2.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_2.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_2.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_2.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_2.ln_2.weight\", \"vit.encoder.layers.encoder_layer_2.ln_2.bias\", \"vit.encoder.layers.encoder_layer_2.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_2.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_2.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_2.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_3.ln_1.weight\", \"vit.encoder.layers.encoder_layer_3.ln_1.bias\", \"vit.encoder.layers.encoder_layer_3.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_3.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_3.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_3.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_3.ln_2.weight\", \"vit.encoder.layers.encoder_layer_3.ln_2.bias\", \"vit.encoder.layers.encoder_layer_3.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_3.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_3.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_3.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_4.ln_1.weight\", \"vit.encoder.layers.encoder_layer_4.ln_1.bias\", \"vit.encoder.layers.encoder_layer_4.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_4.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_4.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_4.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_4.ln_2.weight\", \"vit.encoder.layers.encoder_layer_4.ln_2.bias\", \"vit.encoder.layers.encoder_layer_4.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_4.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_4.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_4.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_5.ln_1.weight\", \"vit.encoder.layers.encoder_layer_5.ln_1.bias\", \"vit.encoder.layers.encoder_layer_5.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_5.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_5.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_5.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_5.ln_2.weight\", \"vit.encoder.layers.encoder_layer_5.ln_2.bias\", \"vit.encoder.layers.encoder_layer_5.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_5.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_5.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_5.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_6.ln_1.weight\", \"vit.encoder.layers.encoder_layer_6.ln_1.bias\", \"vit.encoder.layers.encoder_layer_6.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_6.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_6.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_6.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_6.ln_2.weight\", \"vit.encoder.layers.encoder_layer_6.ln_2.bias\", \"vit.encoder.layers.encoder_layer_6.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_6.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_6.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_6.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_7.ln_1.weight\", \"vit.encoder.layers.encoder_layer_7.ln_1.bias\", \"vit.encoder.layers.encoder_layer_7.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_7.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_7.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_7.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_7.ln_2.weight\", \"vit.encoder.layers.encoder_layer_7.ln_2.bias\", \"vit.encoder.layers.encoder_layer_7.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_7.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_7.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_7.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_8.ln_1.weight\", \"vit.encoder.layers.encoder_layer_8.ln_1.bias\", \"vit.encoder.layers.encoder_layer_8.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_8.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_8.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_8.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_8.ln_2.weight\", \"vit.encoder.layers.encoder_layer_8.ln_2.bias\", \"vit.encoder.layers.encoder_layer_8.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_8.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_8.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_8.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_9.ln_1.weight\", \"vit.encoder.layers.encoder_layer_9.ln_1.bias\", \"vit.encoder.layers.encoder_layer_9.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_9.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_9.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_9.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_9.ln_2.weight\", \"vit.encoder.layers.encoder_layer_9.ln_2.bias\", \"vit.encoder.layers.encoder_layer_9.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_9.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_9.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_9.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_10.ln_1.weight\", \"vit.encoder.layers.encoder_layer_10.ln_1.bias\", \"vit.encoder.layers.encoder_layer_10.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_10.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_10.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_10.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_10.ln_2.weight\", \"vit.encoder.layers.encoder_layer_10.ln_2.bias\", \"vit.encoder.layers.encoder_layer_10.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_10.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_10.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_10.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_11.ln_1.weight\", \"vit.encoder.layers.encoder_layer_11.ln_1.bias\", \"vit.encoder.layers.encoder_layer_11.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_11.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_11.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_11.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_11.ln_2.weight\", \"vit.encoder.layers.encoder_layer_11.ln_2.bias\", \"vit.encoder.layers.encoder_layer_11.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_11.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_11.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_11.mlp.3.bias\", \"vit.encoder.ln.weight\", \"vit.encoder.ln.bias\", \"vit.heads.head.weight\", \"vit.heads.head.bias\". \n\tUnexpected key(s) in state_dict: \"patch_embedding.position_embeddings\", \"patch_embedding.cls_token\", \"patch_embedding.projection.bias\", \"patch_embedding.projection.weight_orig\", \"patch_embedding.projection.weight_mask\", \"transformer_blocks.0.attention.keys.bias\", \"transformer_blocks.0.attention.keys.weight_orig\", \"transformer_blocks.0.attention.keys.weight_mask\", \"transformer_blocks.0.attention.queries.bias\", \"transformer_blocks.0.attention.queries.weight_orig\", \"transformer_blocks.0.attention.queries.weight_mask\", \"transformer_blocks.0.attention.values.bias\", \"transformer_blocks.0.attention.values.weight_orig\", \"transformer_blocks.0.attention.values.weight_mask\", \"transformer_blocks.0.attention.fc_out.bias\", \"transformer_blocks.0.attention.fc_out.weight_orig\", \"transformer_blocks.0.attention.fc_out.weight_mask\", \"transformer_blocks.0.norm1.weight\", \"transformer_blocks.0.norm1.bias\", \"transformer_blocks.0.norm2.weight\", \"transformer_blocks.0.norm2.bias\", \"transformer_blocks.0.feed_forward.0.bias\", \"transformer_blocks.0.feed_forward.0.weight_orig\", \"transformer_blocks.0.feed_forward.0.weight_mask\", \"transformer_blocks.0.feed_forward.2.bias\", \"transformer_blocks.0.feed_forward.2.weight_orig\", \"transformer_blocks.0.feed_forward.2.weight_mask\", \"transformer_blocks.1.attention.keys.bias\", \"transformer_blocks.1.attention.keys.weight_orig\", \"transformer_blocks.1.attention.keys.weight_mask\", \"transformer_blocks.1.attention.queries.bias\", \"transformer_blocks.1.attention.queries.weight_orig\", \"transformer_blocks.1.attention.queries.weight_mask\", \"transformer_blocks.1.attention.values.bias\", \"transformer_blocks.1.attention.values.weight_orig\", \"transformer_blocks.1.attention.values.weight_mask\", \"transformer_blocks.1.attention.fc_out.bias\", \"transformer_blocks.1.attention.fc_out.weight_orig\", \"transformer_blocks.1.attention.fc_out.weight_mask\", \"transformer_blocks.1.norm1.weight\", \"transformer_blocks.1.norm1.bias\", \"transformer_blocks.1.norm2.weight\", \"transformer_blocks.1.norm2.bias\", \"transformer_blocks.1.feed_forward.0.bias\", \"transformer_blocks.1.feed_forward.0.weight_orig\", \"transformer_blocks.1.feed_forward.0.weight_mask\", \"transformer_blocks.1.feed_forward.2.bias\", \"transformer_blocks.1.feed_forward.2.weight_orig\", \"transformer_blocks.1.feed_forward.2.weight_mask\", \"transformer_blocks.2.attention.keys.bias\", \"transformer_blocks.2.attention.keys.weight_orig\", \"transformer_blocks.2.attention.keys.weight_mask\", \"transformer_blocks.2.attention.queries.bias\", \"transformer_blocks.2.attention.queries.weight_orig\", \"transformer_blocks.2.attention.queries.weight_mask\", \"transformer_blocks.2.attention.values.bias\", \"transformer_blocks.2.attention.values.weight_orig\", \"transformer_blocks.2.attention.values.weight_mask\", \"transformer_blocks.2.attention.fc_out.bias\", \"transformer_blocks.2.attention.fc_out.weight_orig\", \"transformer_blocks.2.attention.fc_out.weight_mask\", \"transformer_blocks.2.norm1.weight\", \"transformer_blocks.2.norm1.bias\", \"transformer_blocks.2.norm2.weight\", \"transformer_blocks.2.norm2.bias\", \"transformer_blocks.2.feed_forward.0.bias\", \"transformer_blocks.2.feed_forward.0.weight_orig\", \"transformer_blocks.2.feed_forward.0.weight_mask\", \"transformer_blocks.2.feed_forward.2.bias\", \"transformer_blocks.2.feed_forward.2.weight_orig\", \"transformer_blocks.2.feed_forward.2.weight_mask\", \"classifier.bias\", \"classifier.weight_orig\", \"classifier.weight_mask\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Visualize the results for model_sampling_1.pth\u001b[39;00m\n\u001b[1;32m     38\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_sampling_1.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mvisualize_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Visualize the results for model_sampling_2.pth\u001b[39;00m\n\u001b[1;32m     42\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_sampling_2.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mvisualize_results\u001b[0;34m(model_path, testloader, classes, num_images)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_results\u001b[39m(model_path, testloader, classes, num_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m36\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     net \u001b[38;5;241m=\u001b[39m ViT()\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Get a batch of test images\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ViT:\n\tMissing key(s) in state_dict: \"vit.class_token\", \"vit.conv_proj.weight\", \"vit.conv_proj.bias\", \"vit.encoder.pos_embedding\", \"vit.encoder.layers.encoder_layer_0.ln_1.weight\", \"vit.encoder.layers.encoder_layer_0.ln_1.bias\", \"vit.encoder.layers.encoder_layer_0.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_0.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_0.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_0.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_0.ln_2.weight\", \"vit.encoder.layers.encoder_layer_0.ln_2.bias\", \"vit.encoder.layers.encoder_layer_0.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_0.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_0.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_0.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_1.ln_1.weight\", \"vit.encoder.layers.encoder_layer_1.ln_1.bias\", \"vit.encoder.layers.encoder_layer_1.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_1.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_1.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_1.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_1.ln_2.weight\", \"vit.encoder.layers.encoder_layer_1.ln_2.bias\", \"vit.encoder.layers.encoder_layer_1.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_1.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_1.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_1.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_2.ln_1.weight\", \"vit.encoder.layers.encoder_layer_2.ln_1.bias\", \"vit.encoder.layers.encoder_layer_2.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_2.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_2.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_2.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_2.ln_2.weight\", \"vit.encoder.layers.encoder_layer_2.ln_2.bias\", \"vit.encoder.layers.encoder_layer_2.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_2.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_2.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_2.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_3.ln_1.weight\", \"vit.encoder.layers.encoder_layer_3.ln_1.bias\", \"vit.encoder.layers.encoder_layer_3.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_3.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_3.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_3.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_3.ln_2.weight\", \"vit.encoder.layers.encoder_layer_3.ln_2.bias\", \"vit.encoder.layers.encoder_layer_3.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_3.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_3.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_3.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_4.ln_1.weight\", \"vit.encoder.layers.encoder_layer_4.ln_1.bias\", \"vit.encoder.layers.encoder_layer_4.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_4.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_4.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_4.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_4.ln_2.weight\", \"vit.encoder.layers.encoder_layer_4.ln_2.bias\", \"vit.encoder.layers.encoder_layer_4.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_4.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_4.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_4.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_5.ln_1.weight\", \"vit.encoder.layers.encoder_layer_5.ln_1.bias\", \"vit.encoder.layers.encoder_layer_5.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_5.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_5.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_5.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_5.ln_2.weight\", \"vit.encoder.layers.encoder_layer_5.ln_2.bias\", \"vit.encoder.layers.encoder_layer_5.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_5.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_5.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_5.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_6.ln_1.weight\", \"vit.encoder.layers.encoder_layer_6.ln_1.bias\", \"vit.encoder.layers.encoder_layer_6.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_6.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_6.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_6.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_6.ln_2.weight\", \"vit.encoder.layers.encoder_layer_6.ln_2.bias\", \"vit.encoder.layers.encoder_layer_6.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_6.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_6.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_6.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_7.ln_1.weight\", \"vit.encoder.layers.encoder_layer_7.ln_1.bias\", \"vit.encoder.layers.encoder_layer_7.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_7.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_7.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_7.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_7.ln_2.weight\", \"vit.encoder.layers.encoder_layer_7.ln_2.bias\", \"vit.encoder.layers.encoder_layer_7.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_7.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_7.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_7.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_8.ln_1.weight\", \"vit.encoder.layers.encoder_layer_8.ln_1.bias\", \"vit.encoder.layers.encoder_layer_8.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_8.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_8.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_8.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_8.ln_2.weight\", \"vit.encoder.layers.encoder_layer_8.ln_2.bias\", \"vit.encoder.layers.encoder_layer_8.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_8.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_8.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_8.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_9.ln_1.weight\", \"vit.encoder.layers.encoder_layer_9.ln_1.bias\", \"vit.encoder.layers.encoder_layer_9.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_9.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_9.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_9.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_9.ln_2.weight\", \"vit.encoder.layers.encoder_layer_9.ln_2.bias\", \"vit.encoder.layers.encoder_layer_9.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_9.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_9.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_9.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_10.ln_1.weight\", \"vit.encoder.layers.encoder_layer_10.ln_1.bias\", \"vit.encoder.layers.encoder_layer_10.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_10.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_10.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_10.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_10.ln_2.weight\", \"vit.encoder.layers.encoder_layer_10.ln_2.bias\", \"vit.encoder.layers.encoder_layer_10.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_10.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_10.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_10.mlp.3.bias\", \"vit.encoder.layers.encoder_layer_11.ln_1.weight\", \"vit.encoder.layers.encoder_layer_11.ln_1.bias\", \"vit.encoder.layers.encoder_layer_11.self_attention.in_proj_weight\", \"vit.encoder.layers.encoder_layer_11.self_attention.in_proj_bias\", \"vit.encoder.layers.encoder_layer_11.self_attention.out_proj.weight\", \"vit.encoder.layers.encoder_layer_11.self_attention.out_proj.bias\", \"vit.encoder.layers.encoder_layer_11.ln_2.weight\", \"vit.encoder.layers.encoder_layer_11.ln_2.bias\", \"vit.encoder.layers.encoder_layer_11.mlp.0.weight\", \"vit.encoder.layers.encoder_layer_11.mlp.0.bias\", \"vit.encoder.layers.encoder_layer_11.mlp.3.weight\", \"vit.encoder.layers.encoder_layer_11.mlp.3.bias\", \"vit.encoder.ln.weight\", \"vit.encoder.ln.bias\", \"vit.heads.head.weight\", \"vit.heads.head.bias\". \n\tUnexpected key(s) in state_dict: \"patch_embedding.position_embeddings\", \"patch_embedding.cls_token\", \"patch_embedding.projection.bias\", \"patch_embedding.projection.weight_orig\", \"patch_embedding.projection.weight_mask\", \"transformer_blocks.0.attention.keys.bias\", \"transformer_blocks.0.attention.keys.weight_orig\", \"transformer_blocks.0.attention.keys.weight_mask\", \"transformer_blocks.0.attention.queries.bias\", \"transformer_blocks.0.attention.queries.weight_orig\", \"transformer_blocks.0.attention.queries.weight_mask\", \"transformer_blocks.0.attention.values.bias\", \"transformer_blocks.0.attention.values.weight_orig\", \"transformer_blocks.0.attention.values.weight_mask\", \"transformer_blocks.0.attention.fc_out.bias\", \"transformer_blocks.0.attention.fc_out.weight_orig\", \"transformer_blocks.0.attention.fc_out.weight_mask\", \"transformer_blocks.0.norm1.weight\", \"transformer_blocks.0.norm1.bias\", \"transformer_blocks.0.norm2.weight\", \"transformer_blocks.0.norm2.bias\", \"transformer_blocks.0.feed_forward.0.bias\", \"transformer_blocks.0.feed_forward.0.weight_orig\", \"transformer_blocks.0.feed_forward.0.weight_mask\", \"transformer_blocks.0.feed_forward.2.bias\", \"transformer_blocks.0.feed_forward.2.weight_orig\", \"transformer_blocks.0.feed_forward.2.weight_mask\", \"transformer_blocks.1.attention.keys.bias\", \"transformer_blocks.1.attention.keys.weight_orig\", \"transformer_blocks.1.attention.keys.weight_mask\", \"transformer_blocks.1.attention.queries.bias\", \"transformer_blocks.1.attention.queries.weight_orig\", \"transformer_blocks.1.attention.queries.weight_mask\", \"transformer_blocks.1.attention.values.bias\", \"transformer_blocks.1.attention.values.weight_orig\", \"transformer_blocks.1.attention.values.weight_mask\", \"transformer_blocks.1.attention.fc_out.bias\", \"transformer_blocks.1.attention.fc_out.weight_orig\", \"transformer_blocks.1.attention.fc_out.weight_mask\", \"transformer_blocks.1.norm1.weight\", \"transformer_blocks.1.norm1.bias\", \"transformer_blocks.1.norm2.weight\", \"transformer_blocks.1.norm2.bias\", \"transformer_blocks.1.feed_forward.0.bias\", \"transformer_blocks.1.feed_forward.0.weight_orig\", \"transformer_blocks.1.feed_forward.0.weight_mask\", \"transformer_blocks.1.feed_forward.2.bias\", \"transformer_blocks.1.feed_forward.2.weight_orig\", \"transformer_blocks.1.feed_forward.2.weight_mask\", \"transformer_blocks.2.attention.keys.bias\", \"transformer_blocks.2.attention.keys.weight_orig\", \"transformer_blocks.2.attention.keys.weight_mask\", \"transformer_blocks.2.attention.queries.bias\", \"transformer_blocks.2.attention.queries.weight_orig\", \"transformer_blocks.2.attention.queries.weight_mask\", \"transformer_blocks.2.attention.values.bias\", \"transformer_blocks.2.attention.values.weight_orig\", \"transformer_blocks.2.attention.values.weight_mask\", \"transformer_blocks.2.attention.fc_out.bias\", \"transformer_blocks.2.attention.fc_out.weight_orig\", \"transformer_blocks.2.attention.fc_out.weight_mask\", \"transformer_blocks.2.norm1.weight\", \"transformer_blocks.2.norm1.bias\", \"transformer_blocks.2.norm2.weight\", \"transformer_blocks.2.norm2.bias\", \"transformer_blocks.2.feed_forward.0.bias\", \"transformer_blocks.2.feed_forward.0.weight_orig\", \"transformer_blocks.2.feed_forward.0.weight_mask\", \"transformer_blocks.2.feed_forward.2.bias\", \"transformer_blocks.2.feed_forward.2.weight_orig\", \"transformer_blocks.2.feed_forward.2.weight_mask\", \"classifier.bias\", \"classifier.weight_orig\", \"classifier.weight_mask\". "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from models import ViT\n",
    "\n",
    "# Define the data transformation for CIFAR-10\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the images\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load the CIFAR-10 dataset - test\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"data\", train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the classes for CIFAR-10\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")\n",
    "\n",
    "# Visualize the results for model_sampling_1.pth\n",
    "model_path = os.path.join(\".\", \"model_sampling_1.pth\")\n",
    "visualize_results(model_path, testloader, classes)\n",
    "\n",
    "# Visualize the results for model_sampling_2.pth\n",
    "model_path = os.path.join(\".\", \"model_sampling_2.pth\")\n",
    "visualize_results(model_path, testloader, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-cw1-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
