{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Minibatch Gradient Descent for Linear Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the polynomial_fun() function\n",
    "def polynomial_fun(w, x):\n",
    "    \"\"\"\n",
    "    Calculates the value of a polynomial function at a given point.\n",
    "\n",
    "    Parameters:\n",
    "    w (torch.Tensor): Coefficients of the polynomial function.\n",
    "    x (torch.Tensor): The point(s) at which to evaluate the polynomial function.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The value(s) of the polynomial function at the given point(s).\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if w and x are tensors\n",
    "    if not isinstance(w, torch.Tensor):\n",
    "        raise TypeError(\"w must be a tensor\")\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        raise TypeError(\"x must be a tensor\")\n",
    "\n",
    "    M = w.shape[0]\n",
    "    terms = [w[i] * torch.pow(x, i) for i in range(M)]\n",
    "    y = torch.stack(terms, dim=0).sum(dim=0)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polynomial_ls(x, t, M):\n",
    "    \"\"\"\n",
    "    Fits a polynomial function to a set of data points using least squares.\n",
    "\n",
    "    Parameters:\n",
    "    x (array or list): The x-coordinates of the data points.\n",
    "    t (array or list): The target values of the data points.\n",
    "    M (int): The degree of the polynomial to fit to the data.\n",
    "\n",
    "    Returns:\n",
    "    array: The coefficients of the polynomial function that best fits the data.\n",
    "    \"\"\"\n",
    "    # Convert x and t to tensors if they are lists\n",
    "    if isinstance(x, list):\n",
    "        x = torch.tensor(x)\n",
    "    if isinstance(t, list):\n",
    "        t = torch.tensor(t)\n",
    "\n",
    "    # Make sure the inputs are valid\n",
    "    ## Type\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        raise TypeError(\"x must be a tensor or list\")\n",
    "    if not isinstance(t, torch.Tensor):\n",
    "        raise TypeError(\"t must be a tensor or list\")\n",
    "    if not isinstance(M, int):\n",
    "        raise TypeError(\"M must be an integer\")\n",
    "\n",
    "    # Fit the polynomial function to the data\n",
    "    A = torch.vander(x, M + 1)\n",
    "    w = torch.linalg.lstsq(A, t.unsqueeze(1)).solution.squeeze(1)\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polynomial_sgd(x, t, M, learning_rate, minibatch_size, period=100):\n",
    "    \"\"\"\n",
    "    Fits a polynomial function to a set of data points using stochastic gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    x (torch.Tensor): The x-coordinates of the data points.\n",
    "    t (torch.Tensor): The target values of the data points.\n",
    "    M (int): The degree of the polynomial to fit to the data.\n",
    "    learning_rate (float): The learning rate to use for the SGD algorithm.\n",
    "    minibatch_size (int): The number of data points to use in each minibatch.\n",
    "    period (int): The number of epochs to run the SGD algorithm.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The coefficients of the polynomial function that best fits the data.\n",
    "\n",
    "    Prints:\n",
    "    float: The loss value at each 10% epoch.\n",
    "    \"\"\"\n",
    "    # Make sure the inputs are valid\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        raise TypeError(\"x must be a tensor\")\n",
    "    if not isinstance(t, torch.Tensor):\n",
    "        raise TypeError(\"t must be a tensor\")\n",
    "    if not isinstance(M, int):\n",
    "        raise TypeError(\"M must be an integer\")\n",
    "    if not isinstance(learning_rate, float):\n",
    "        raise TypeError(\"learning_rate must be a float\")\n",
    "    if not isinstance(minibatch_size, int):\n",
    "        raise TypeError(\"minibatch_size must be an integer\")\n",
    "\n",
    "    # Fit the polynomial function to the data\n",
    "    w = torch.randn(M + 1, requires_grad=True)\n",
    "    for epoch in range(period):\n",
    "        indices = torch.randperm(x.shape[0])[:minibatch_size]\n",
    "        x_batch = x[indices]\n",
    "        t_batch = t[indices]\n",
    "        A = torch.vander(x_batch, M + 1)\n",
    "        y = torch.mv(A, w)\n",
    "        loss = torch.mean((y - t_batch) ** 2)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w -= learning_rate * w.grad\n",
    "            w.grad.zero_()\n",
    "\n",
    "        if epoch % (period // 10) == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Generating the training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the input data\n",
    "M = 2\n",
    "w_T = torch.tensor([1, 2, 3])\n",
    "w = w_T.T\n",
    "\n",
    "\n",
    "# Generate training set\n",
    "torch.manual_seed(0)\n",
    "x_train = torch.rand(20) * 40 - 20  # random values between -20 and 20\n",
    "y_train_gt = polynomial_fun(w, x_train)  # ground truth\n",
    "t_train = y_train_gt + torch.randn(20) * 0.5\n",
    "\n",
    "# Generate test set\n",
    "x_test = torch.rand(10) * 40 - 20\n",
    "y_test_gt = polynomial_fun(w, x_test)  # ground truth\n",
    "t_test = y_test_gt + torch.randn(10) * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: tensor([ -0.1497,  10.7289, -16.4609, -14.7188,  -7.7031,   5.3631,  -0.3963,\n",
      "         15.8578,  -1.7749,   5.2923,  -6.0443,  -3.9313, -19.1070, -13.2456,\n",
      "         -8.2445,   0.7409,   7.9067,  12.0005, -13.5588,  -8.7093])\n",
      "y_train_gt: tensor([7.6779e-01, 3.6778e+02, 7.8096e+02, 6.2149e+02, 1.6361e+02, 9.8016e+01,\n",
      "        6.7855e-01, 7.8712e+02, 6.9008e+00, 9.5608e+01, 9.8511e+01, 3.9503e+01,\n",
      "        1.0580e+03, 5.0085e+02, 1.8842e+02, 4.1284e+00, 2.0436e+02, 4.5703e+02,\n",
      "        5.2541e+02, 2.1113e+02])\n",
      "t_train: tensor([1.0672e+00, 3.6701e+02, 7.8079e+02, 6.2242e+02, 1.6350e+02, 9.7645e+01,\n",
      "        9.5991e-01, 7.8725e+02, 6.8139e+00, 9.5269e+01, 9.8980e+01, 3.9747e+01,\n",
      "        1.0586e+03, 5.0089e+02, 1.8782e+02, 4.1260e+00, 2.0410e+02, 4.5688e+02,\n",
      "        5.2462e+02, 2.1199e+02])\n",
      "------\n",
      "x_test: tensor([  3.7269, -15.5061, -13.8617, -10.3317,   9.0495,   8.0432, -11.8470,\n",
      "          6.0421,  10.9794,  -2.5243])\n",
      "y_test_gt: tensor([ 50.1227, 691.3062, 549.7187, 300.5670, 264.7772, 211.1661, 398.3636,\n",
      "        122.6067, 384.6032,  15.0683])\n",
      "t_test: tensor([ 50.4515, 691.7755, 550.2845, 300.2442, 263.8920, 211.2732, 398.0945,\n",
      "        122.9007, 385.4062,  15.2822])\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train: {x_train}\")\n",
    "print(f\"y_train_gt: {y_train_gt}\")\n",
    "print(f\"t_train: {t_train}\")\n",
    "\n",
    "print(\"------\")\n",
    "\n",
    "\n",
    "print(f\"x_test: {x_test}\")\n",
    "print(f\"y_test_gt: {y_test_gt}\")\n",
    "print(f\"t_test: {t_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Fitting the training set - LeastSquare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEAST SQUARE EVALUATION\n",
      "M=2\n",
      "a) Difference between observed training data and true polynomial curve\n",
      "Mean difference between observed training data and true polynomial curve: 0.009434586390852928\n",
      "Standard deviation of difference between observed training data and true polynomial curve: 0.48098304867744446\n",
      "b) Difference between LS-predicted values and true polynomial curve\n",
      "Mean difference between LS-predicted values and true polynomial curve: -212.5789337158203\n",
      "Standard deviation of difference between LS-predicted values and true polynomial curve: 217.66619873046875\n",
      "---------------------------------------------------\n",
      "M=3\n",
      "a) Difference between observed training data and true polynomial curve\n",
      "Mean difference between observed training data and true polynomial curve: 0.009434586390852928\n",
      "Standard deviation of difference between observed training data and true polynomial curve: 0.48098304867744446\n",
      "b) Difference between LS-predicted values and true polynomial curve\n",
      "Mean difference between LS-predicted values and true polynomial curve: -751.3699951171875\n",
      "Standard deviation of difference between LS-predicted values and true polynomial curve: 2249.113037109375\n",
      "---------------------------------------------------\n",
      "M=4\n",
      "a) Difference between observed training data and true polynomial curve\n",
      "Mean difference between observed training data and true polynomial curve: 0.009434586390852928\n",
      "Standard deviation of difference between observed training data and true polynomial curve: 0.48098304867744446\n",
      "b) Difference between LS-predicted values and true polynomial curve\n",
      "Mean difference between LS-predicted values and true polynomial curve: 22818.490234375\n",
      "Standard deviation of difference between LS-predicted values and true polynomial curve: 35477.3515625\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Fit polynomial function using fit_polynomial_lstsq\n",
    "\n",
    "## Setting\n",
    "M_values = [2, 3, 4]\n",
    "\n",
    "## Initalising\n",
    "w_hat_values = []\n",
    "y_train_values = []\n",
    "y_test_values = []\n",
    "lstsq_fit_times = []\n",
    "lstsq_train_times = []\n",
    "lstsq_test_times = []\n",
    "\n",
    "## Looping\n",
    "print(f\"LEAST SQUARE EVALUATION\")\n",
    "for M in M_values:\n",
    "    print(f\"M={M}\")\n",
    "\n",
    "    # Compute the optimum weight vector\n",
    "    start_time = time.time()\n",
    "    w_hat = fit_polynomial_ls(x_train, t_train, M)\n",
    "    end_time = time.time()\n",
    "    lstsq_fit_times.append(end_time - start_time)\n",
    "    w_hat_values.append(w_hat)\n",
    "\n",
    "    # Compute predicted target values for training set\n",
    "    start_time = time.time()\n",
    "    y_train = polynomial_fun(w_hat, x_train)\n",
    "    end_time = time.time()\n",
    "    lstsq_train_times.append(end_time - start_time)\n",
    "    y_train_values.append(y_train)\n",
    "\n",
    "    # Compute predicted target values for test set\n",
    "    start_time = time.time()\n",
    "    y_test = polynomial_fun(w_hat, x_test)\n",
    "    end_time = time.time()\n",
    "    lstsq_test_times.append(end_time - start_time)\n",
    "    y_test_values.append(y_test)\n",
    "\n",
    "    #! TODO: Meansquare\n",
    "    # Reporting a\n",
    "\n",
    "    # Calculate the mean and standard deviation of the difference between the observed training data and the true polynomial curve\n",
    "    print(f\"a) Difference between observed training data and true polynomial curve\")\n",
    "    diff_train = t_train - polynomial_fun(w, x_train)\n",
    "    mean_diff_train = diff_train.mean()\n",
    "    std_diff_train = diff_train.std()\n",
    "    print(\n",
    "        f\"Mean difference between observed training data and true polynomial curve: {mean_diff_train}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Standard deviation of difference between observed training data and true polynomial curve: {std_diff_train}\"\n",
    "    )\n",
    "\n",
    "    # Reporting b\n",
    "    # Calculate the mean and standard deviation of the difference between the LS-predicted values and the true polynomial curve\n",
    "    print(f\"b) Difference between LS-predicted values and true polynomial curve\")\n",
    "    diff_ls_predicted = y_train - polynomial_fun(w, x_train)\n",
    "    mean_diff_ls_predicted = diff_ls_predicted.mean()\n",
    "    std_diff_ls_predicted = diff_ls_predicted.std()\n",
    "    print(\n",
    "        f\"Mean difference between LS-predicted values and true polynomial curve: {mean_diff_ls_predicted}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Standard deviation of difference between LS-predicted values and true polynomial curve: {std_diff_ls_predicted}\"\n",
    "    )\n",
    "\n",
    "    print(\"---------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Fitting the training set - SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD EVALUATION\n",
      "M=2\n",
      "Epoch 0: Loss = 483663.90625\n",
      "Epoch 10: Loss = 335765.40625\n",
      "Epoch 20: Loss = 304490.53125\n",
      "Epoch 30: Loss = 886467.375\n",
      "Epoch 40: Loss = 9856.11328125\n",
      "Epoch 50: Loss = 191692.8125\n",
      "Epoch 60: Loss = 459068.03125\n",
      "Epoch 70: Loss = 115772.5625\n",
      "Epoch 80: Loss = 595245.5\n",
      "Epoch 90: Loss = 1449.019287109375\n",
      "Difference between observed training data and true polynomial curve\n",
      "Mean difference between SGD-predicted values and true polynomial curve for M=2: -306.2291564941406\n",
      "Standard deviation of difference between SGD-predicted values and true polynomial curve for M=2: 306.3013610839844\n",
      "Difference between the SGD-predicted values and the true polynomial curve for the test set\n",
      "Mean difference between SGD-predicted values and true polynomial curve for M=2 (test set): -293.73138427734375\n",
      "Standard deviation of difference between SGD-predicted values and true polynomial curve for M=2 (test set): 214.8321990966797\n",
      "---------------------------------------------------\n",
      "M=3\n",
      "Epoch 0: Loss = 1070272.0\n",
      "Epoch 10: Loss = 33348.7421875\n",
      "Epoch 20: Loss = 9121.9970703125\n",
      "Epoch 30: Loss = 284.3482360839844\n",
      "Epoch 40: Loss = 107019.765625\n",
      "Epoch 50: Loss = 47719.65625\n",
      "Epoch 60: Loss = 51112.51171875\n",
      "Epoch 70: Loss = 96362.1015625\n",
      "Epoch 80: Loss = 29426.05078125\n",
      "Epoch 90: Loss = 49804.96875\n",
      "Difference between observed training data and true polynomial curve\n",
      "Mean difference between SGD-predicted values and true polynomial curve for M=3: -266.4220275878906\n",
      "Standard deviation of difference between SGD-predicted values and true polynomial curve for M=3: 457.348876953125\n",
      "Difference between the SGD-predicted values and the true polynomial curve for the test set\n",
      "Mean difference between SGD-predicted values and true polynomial curve for M=3 (test set): -252.9086456298828\n",
      "Standard deviation of difference between SGD-predicted values and true polynomial curve for M=3 (test set): 336.02935791015625\n",
      "---------------------------------------------------\n",
      "M=4\n",
      "Epoch 0: Loss = 631979520.0\n",
      "Epoch 10: Loss = 2.5376775093916214e+23\n",
      "Epoch 20: Loss = inf\n",
      "Epoch 30: Loss = inf\n",
      "Epoch 40: Loss = nan\n",
      "Epoch 50: Loss = nan\n",
      "Epoch 60: Loss = nan\n",
      "Epoch 70: Loss = nan\n",
      "Epoch 80: Loss = nan\n",
      "Epoch 90: Loss = nan\n",
      "Difference between observed training data and true polynomial curve\n",
      "Mean difference between SGD-predicted values and true polynomial curve for M=4: nan\n",
      "Standard deviation of difference between SGD-predicted values and true polynomial curve for M=4: nan\n",
      "Difference between the SGD-predicted values and the true polynomial curve for the test set\n",
      "Mean difference between SGD-predicted values and true polynomial curve for M=4 (test set): nan\n",
      "Standard deviation of difference between SGD-predicted values and true polynomial curve for M=4 (test set): nan\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Assuming you have x_train, t_train, x_test, and t_test defined\n",
    "\n",
    "# Hyperparameters\n",
    "M_values = [2, 3, 4]\n",
    "learning_rate = 1e-8\n",
    "minibatch_size = 2\n",
    "epochs = 100\n",
    "\n",
    "# Fit the true polynomial function using least squares\n",
    "w = fit_polynomial_ls(x_train, t_train, M_values[0])\n",
    "\n",
    "# Fit polynomial function using fit_polynomial_sgd\n",
    "sgd_w_hat_values = []\n",
    "sgd_y_train_values = []\n",
    "sgd_y_test_values = []\n",
    "sgd_fit_times = []\n",
    "sgd_train_times = []\n",
    "sgd_test_times = []\n",
    "\n",
    "print(\"SGD EVALUATION\")\n",
    "for M in M_values:\n",
    "    print(f\"M={M}\")\n",
    "\n",
    "    # Fit the polynomial function using SGD\n",
    "    # => sgd_w_hat\n",
    "    start_time = time.time()\n",
    "    sgd_w_hat = fit_polynomial_sgd(\n",
    "        x_train, t_train, M, learning_rate, minibatch_size, epochs\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    sgd_fit_times.append(end_time - start_time)\n",
    "    sgd_w_hat_values.append(sgd_w_hat)\n",
    "\n",
    "    # Compute predicted target values for training set using SGD\n",
    "    # => sgd_y_train\n",
    "    start_time = time.time()\n",
    "    sgd_y_train = polynomial_fun(sgd_w_hat, x_train)\n",
    "    end_time = time.time()\n",
    "    sgd_train_times.append(end_time - start_time)\n",
    "    sgd_y_train_values.append(sgd_y_train)\n",
    "\n",
    "    # Compute predicted target values for test set using SGD\n",
    "    # => sgd_y_test\n",
    "    start_time = time.time()\n",
    "    sgd_y_test = polynomial_fun(sgd_w_hat, x_test)\n",
    "    end_time = time.time()\n",
    "    sgd_test_times.append(end_time - start_time)\n",
    "    sgd_y_test_values.append(sgd_y_test)\n",
    "\n",
    "    # Difference between the SGD-predicted values and the true polynomial curve\n",
    "    print(\"Difference between observed training data and true polynomial curve\")\n",
    "    diff_sgd_predicted_train = sgd_y_train - polynomial_fun(w, x_train)\n",
    "    mean_diff_sgd_predicted_train = diff_sgd_predicted_train.mean()\n",
    "    std_diff_sgd_predicted_train = diff_sgd_predicted_train.std()\n",
    "    print(\n",
    "        f\"Mean difference between SGD-predicted values and true polynomial curve for M={M}: {mean_diff_sgd_predicted_train}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Standard deviation of difference between SGD-predicted values and true polynomial curve for M={M}: {std_diff_sgd_predicted_train}\"\n",
    "    )\n",
    "\n",
    "    # Difference between the SGD-predicted values and the true polynomial curve for the test set\n",
    "    print(\n",
    "        \"Difference between the SGD-predicted values and the true polynomial curve for the test set\"\n",
    "    )\n",
    "    diff_sgd_predicted_test = sgd_y_test - polynomial_fun(w, x_test)\n",
    "    mean_diff_sgd_predicted_test = diff_sgd_predicted_test.mean()\n",
    "    std_diff_sgd_predicted_test = diff_sgd_predicted_test.std()\n",
    "    print(\n",
    "        f\"Mean difference between SGD-predicted values and true polynomial curve for M={M} (test set): {mean_diff_sgd_predicted_test}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Standard deviation of difference between SGD-predicted values and true polynomial curve for M={M} (test set): {std_diff_sgd_predicted_test}\"\n",
    "    )\n",
    "\n",
    "    print(\"---------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEED COMPARISON\n",
      "Least squares\n",
      "Time spent in fitting using least squares: 0.0005087852478027344 seconds\n",
      "Time spent in training using least squares: 0.0002117156982421875 seconds\n",
      "-----------------\n",
      "SGD times\n",
      "Time spent in fitting using SGD: 0.023846149444580078 seconds\n",
      "Time spent in training using SGD: 0.0001971721649169922 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compare the speed of the two methods\n",
    "print(f\"SPEED COMPARISON\")\n",
    "print(f\"Least squares\")\n",
    "print(f\"Time spent in fitting using least squares: {np.sum(lstsq_fit_times)} seconds\")\n",
    "print(\n",
    "    f\"Time spent in training using least squares: {np.sum(lstsq_train_times)} seconds\"\n",
    ")\n",
    "print(\"-----------------\")\n",
    "print(\"SGD times\")\n",
    "print(f\"Time spent in fitting using SGD: {np.sum(sgd_fit_times)} seconds\")\n",
    "print(f\"Time spent in training using SGD: {np.sum(sgd_train_times)} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimise with M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def fit_polynomial_sgd_M(x, t, learning_rate, minibatch_size, epochs, M_init=2):\n",
    "    \"\"\"\n",
    "    Fits a polynomial function to a set of data points using stochastic gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    x (torch.Tensor): The x-coordinates of the data points.\n",
    "    t (torch.Tensor): The target values of the data points.\n",
    "    learning_rate (float): The learning rate to use for the SGD algorithm.\n",
    "    minibatch_size (int): The number of data points to use in each minibatch.\n",
    "    epochs (int): The number of epochs to run the SGD algorithm.\n",
    "    M_init (int): The initial value of the polynomial degree M.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The coefficients of the polynomial function that best fits the data.\n",
    "    int: The optimized value of the polynomial degree M.\n",
    "\n",
    "    Prints:\n",
    "    float: The loss value at each 10% epoch.\n",
    "    \"\"\"\n",
    "    # Make sure the inputs are valid\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        raise TypeError(\"x must be a tensor\")\n",
    "    if not isinstance(t, torch.Tensor):\n",
    "        raise TypeError(\"t must be a tensor\")\n",
    "    if not isinstance(learning_rate, float):\n",
    "        raise TypeError(\"learning_rate must be a float\")\n",
    "    if not isinstance(minibatch_size, int):\n",
    "        raise TypeError(\"minibatch_size must be an integer\")\n",
    "    if not isinstance(epochs, int):\n",
    "        raise TypeError(\"epochs must be an integer\")\n",
    "    if not isinstance(M_init, int):\n",
    "        raise TypeError(\"M_init must be an integer\")\n",
    "\n",
    "    # Fit the polynomial function to the data\n",
    "    w = torch.randn(M_init + 1, requires_grad=True)\n",
    "    M = torch.tensor(float(M_init), requires_grad=True)  # Convert M_init to float\n",
    "    optimizer = torch.optim.SGD([w, M], lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        indices = torch.randperm(x.shape[0])[:minibatch_size]\n",
    "        x_batch = x[indices]\n",
    "        t_batch = t[indices]\n",
    "        A = torch.vander(x_batch, int(M.item()) + 1)\n",
    "        y = torch.mv(A, w)\n",
    "        loss = torch.mean((y - t_batch) ** 2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % (epochs // 10) == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "\n",
    "    M_optimized = int(M.item())\n",
    "    return w, M_optimized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 453697.5\n",
      "Epoch 100: Loss = nan\n",
      "Epoch 200: Loss = nan\n",
      "Epoch 300: Loss = nan\n",
      "Epoch 400: Loss = nan\n",
      "Epoch 500: Loss = nan\n",
      "Epoch 600: Loss = nan\n",
      "Epoch 700: Loss = nan\n",
      "Epoch 800: Loss = nan\n",
      "Epoch 900: Loss = nan\n",
      "Optimized value of M: 2\n",
      "Difference between observed training data and true polynomial curve\n",
      "Mean difference between SGD-predicted values and true polynomial curve: nan\n",
      "Standard deviation of difference between SGD-predicted values and true polynomial curve: nan\n",
      "Difference between the SGD-predicted values and the true polynomial curve for the test set\n",
      "Mean difference between SGD-predicted values and true polynomial curve (test set): nan\n",
      "Standard deviation of difference between SGD-predicted values and true polynomial curve (test set): nan\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have x_train, t_train, x_test, and t_test defined\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "minibatch_size = 2\n",
    "epochs = 1000\n",
    "\n",
    "# Fit the true polynomial function using least squares\n",
    "w_true = fit_polynomial_ls(x_train, t_train, M_values[0])\n",
    "\n",
    "# Fit the polynomial function using fit_polynomial_sgd\n",
    "sgd_w_hat, sgd_M_optimized = fit_polynomial_sgd_M(\n",
    "    x_train, t_train, learning_rate, minibatch_size, epochs\n",
    ")\n",
    "\n",
    "# Compute predicted target values for training set using SGD\n",
    "sgd_y_train = polynomial_fun(sgd_w_hat, x_train)\n",
    "\n",
    "# Compute predicted target values for test set using SGD\n",
    "sgd_y_test = polynomial_fun(sgd_w_hat, x_test)\n",
    "\n",
    "# Difference between the SGD-predicted values and the true polynomial curve\n",
    "print(f\"Optimized value of M: {sgd_M_optimized}\")\n",
    "\n",
    "print(\"Difference between observed training data and true polynomial curve\")\n",
    "diff_sgd_predicted_train = sgd_y_train - polynomial_fun(w_true, x_train)\n",
    "mean_diff_sgd_predicted_train = diff_sgd_predicted_train.mean()\n",
    "std_diff_sgd_predicted_train = diff_sgd_predicted_train.std()\n",
    "print(\n",
    "    f\"Mean difference between SGD-predicted values and true polynomial curve: {mean_diff_sgd_predicted_train}\"\n",
    ")\n",
    "print(\n",
    "    f\"Standard deviation of difference between SGD-predicted values and true polynomial curve: {std_diff_sgd_predicted_train}\"\n",
    ")\n",
    "\n",
    "# Difference between the SGD-predicted values and the true polynomial curve for the test set\n",
    "print(\n",
    "    \"Difference between the SGD-predicted values and the true polynomial curve for the test set\"\n",
    ")\n",
    "diff_sgd_predicted_test = sgd_y_test - polynomial_fun(w_true, x_test)\n",
    "mean_diff_sgd_predicted_test = diff_sgd_predicted_test.mean()\n",
    "std_diff_sgd_predicted_test = diff_sgd_predicted_test.std()\n",
    "print(\n",
    "    f\"Mean difference between SGD-predicted values and true polynomial curve (test set): {mean_diff_sgd_predicted_test}\"\n",
    ")\n",
    "print(\n",
    "    f\"Standard deviation of difference between SGD-predicted values and true polynomial curve (test set): {std_diff_sgd_predicted_test}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying M=2\n",
      "Epoch 0: Loss = 7341.1220703125\n",
      "Epoch 10: Loss = 3147.911865234375\n",
      "Epoch 20: Loss = 327439.875\n",
      "Epoch 30: Loss = 4.4719648361206055\n",
      "Epoch 40: Loss = 187536.890625\n",
      "Epoch 50: Loss = 104310.265625\n",
      "Epoch 60: Loss = 78789.4609375\n",
      "Epoch 70: Loss = 101940.3125\n",
      "Epoch 80: Loss = 261221.375\n",
      "Epoch 90: Loss = 158084.90625\n",
      "Trying M=3\n",
      "Epoch 0: Loss = 4210033.0\n",
      "Epoch 10: Loss = 7880.95849609375\n",
      "Epoch 20: Loss = 252684.359375\n",
      "Epoch 30: Loss = 356505.46875\n",
      "Epoch 40: Loss = 9718.95703125\n",
      "Epoch 50: Loss = 143256.984375\n",
      "Epoch 60: Loss = 289911.75\n",
      "Epoch 70: Loss = 9039.970703125\n",
      "Epoch 80: Loss = 178830.65625\n",
      "Epoch 90: Loss = 11050.654296875\n",
      "Trying M=4\n",
      "Epoch 0: Loss = 166214.078125\n",
      "Epoch 10: Loss = 1.007356517738948e+25\n",
      "Epoch 20: Loss = inf\n",
      "Epoch 30: Loss = inf\n",
      "Epoch 40: Loss = nan\n",
      "Epoch 50: Loss = nan\n",
      "Epoch 60: Loss = nan\n",
      "Epoch 70: Loss = nan\n",
      "Epoch 80: Loss = nan\n",
      "Epoch 90: Loss = nan\n",
      "Trying M=5\n",
      "Epoch 0: Loss = 13773918208.0\n",
      "Epoch 10: Loss = inf\n",
      "Epoch 20: Loss = nan\n",
      "Epoch 30: Loss = nan\n",
      "Epoch 40: Loss = nan\n",
      "Epoch 50: Loss = nan\n",
      "Epoch 60: Loss = nan\n",
      "Epoch 70: Loss = nan\n",
      "Epoch 80: Loss = nan\n",
      "Epoch 90: Loss = nan\n",
      "Optimized value of M: 3\n",
      "Mean difference between SGD-predicted values and true polynomial curve (training set): -119.3194351196289\n",
      "Mean difference between SGD-predicted values and true polynomial curve (test set): -118.93843078613281\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have x_train, t_train, x_test, and t_test defined\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-8\n",
    "minibatch_size = 2\n",
    "epochs = 100\n",
    "M_values = [2, 3, 4, 5]  # List of M values to try\n",
    "\n",
    "# Fit the true polynomial function using least squares\n",
    "w_true = fit_polynomial_ls(x_train, t_train, M_values[0])\n",
    "\n",
    "best_M = None\n",
    "best_w_hat = None\n",
    "best_train_diff = float(\"inf\")\n",
    "best_test_diff = float(\"inf\")\n",
    "\n",
    "for M in M_values:\n",
    "    print(f\"Trying M={M}\")\n",
    "\n",
    "    # Fit the polynomial function using fit_polynomial_sgd\n",
    "    sgd_w_hat = fit_polynomial_sgd(\n",
    "        x_train, t_train, M, learning_rate, minibatch_size, epochs\n",
    "    )\n",
    "\n",
    "    # Compute predicted target values for training set using SGD\n",
    "    sgd_y_train = polynomial_fun(sgd_w_hat, x_train)\n",
    "\n",
    "    # Compute predicted target values for test set using SGD\n",
    "    sgd_y_test = polynomial_fun(sgd_w_hat, x_test)\n",
    "\n",
    "    # Difference between the SGD-predicted values and the true polynomial curve\n",
    "    diff_sgd_predicted_train = sgd_y_train - polynomial_fun(w_true, x_train)\n",
    "    mean_diff_sgd_predicted_train = diff_sgd_predicted_train.mean().item()\n",
    "\n",
    "    diff_sgd_predicted_test = sgd_y_test - polynomial_fun(w_true, x_test)\n",
    "    mean_diff_sgd_predicted_test = diff_sgd_predicted_test.mean().item()\n",
    "\n",
    "    if (\n",
    "        mean_diff_sgd_predicted_train < best_train_diff\n",
    "        and mean_diff_sgd_predicted_test < best_test_diff\n",
    "    ):\n",
    "        best_M = M\n",
    "        best_w_hat = sgd_w_hat\n",
    "        best_train_diff = mean_diff_sgd_predicted_train\n",
    "        best_test_diff = mean_diff_sgd_predicted_test\n",
    "\n",
    "print(f\"Optimized value of M: {best_M}\")\n",
    "print(\n",
    "    f\"Mean difference between SGD-predicted values and true polynomial curve (training set): {best_train_diff}\"\n",
    ")\n",
    "print(\n",
    "    f\"Mean difference between SGD-predicted values and true polynomial curve (test set): {best_test_diff}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp0197-cw1-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
